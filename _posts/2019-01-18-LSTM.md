---
title: "TensorFlow - not another introduction II"
layout: post
date: 2019-01-18
image: /assets/images/tensorflow.jpg
headerImage: true
tag:
- Machine Learning
- Neural Network
- Google
- Computation Framework
category: blog
author: Alan Yu
description: An introduction to Google's open-source neural network framework tensorflow

---

## Background 

In this post we will introduce a more interesting, state-of-art neural network model to the readers: Long-short-term-memory Network.

_Long-short-term-memory network_ or _LSTM network_ for short is a special case in Recurrent Neural Network that tries to avoid the "vanishing gradient problem." 

Lost in the jargons? Let's take a step back and try to understand all the words in the above paragraph. <a href="https://en.wikipedia.org/wiki/Recurrent_neural_network">Recurrent Neural Network</a> is a type of neural network that specializes in processing sequences of inputs via its storage of the internal states. 

A typical Recurrent Neural Network looks like the following.








