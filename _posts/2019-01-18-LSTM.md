---
title: "TensorFlow - not another introduction II"
layout: post
date: 2019-01-18
image: /assets/images/tensorflow.jpg
headerImage: true
tag:
- Machine Learning
- Neural Network
- Google
- Computation Framework
category: blog
author: Alan Yu
description: An introduction to Google's open-source neural network framework tensorflow

---

## Background 

In this post we will introduce a more interesting, state-of-art neural network model to the readers: Long-short-term-memory Network.

_Long-short-term-memory network_ or _LSTM network_ for short is a special case in Recurrent Neural Network that tries to avoid the "vanishing gradient problem." 

Lost in the jargons? Let's take a step back and try to understand all the words in the above paragraph. <a href="https://en.wikipedia.org/wiki/Recurrent_neural_network">Recurrent Neural Network</a> is a type of neural network that specializes in processing sequences of inputs via its storage of the internal states. 

A typical Recurrent Neural Network looks like the following.

<img src="https://raw.githubusercontent.com/AlanFermat/AlanFermat.github.io/master/assets/images/lstm1.png">

## Vanishing Gradient Problem

If we unroll the network above, we will see something like this,
<img src="https://raw.githubusercontent.com/AlanFermat/AlanFermat.github.io/master/assets/images/lstm6.png">

Recall how we calculate the derivative in the backpropagation from an <a href="https://imalanyu.com/Neural-Network/">early post</a>. 

<div class="side-by-side">
	<div class="toleft">
		<div style="margin-bottom: 100pt">
			<img class="image" src="https://raw.githubusercontent.com/AlanFermat/AlanFermat.github.io/master/assets/images/lstm3.png" alt="Alt Text">
			<figcaption class="caption">Derivative of logistic activation function</figcaption>
		</div>
		<div padding-bottom="10">
			<img class="image" src="https://raw.githubusercontent.com/AlanFermat/AlanFermat.github.io/master/assets/images/lstm2.png" alt="Alt Text">
			<figcaption class="caption">Illustration for calculating derivatives in backprop</figcaption>
		</div>
	</div>
	<div class="toright">
		<p>Note that the derivative of loss with respect to activation function is often less than 1.</p>
		<p>Think about the derivatice with respect to the logistic function on the left for example, we know that maximum value is reached when the activation function takes the value of 0.5 which is less than 1.</p>
		<p>If we have a deep RNN and apply that multiplier constantly as we backprop the errors to the input layer, the derived gradient is approaching zero. </p>
		<p>The error from the output layer may not be able to influence the upfront layers as they are too far.</p>
		<p>This pattern would make the training stagnant as no correction would be made in the upfront layers due to the errors in the output.</p>
	</div>
</div>

You can see more detailed explanations on the vanishing gradient problem below.

<iframe width="560" height="315" src="https://www.youtube.com/embed/qO_NLVjD6zE" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

## LSTM

Walla! Here comes the savior -- LSTM network. In LSTM net, we don't push the long term memory through the activation function, i.e. we don't backprop the long term memory of an error all the way to the input layer. Thus, we don't have a vanishing gradient problem!

Let's look at the LSTM "modules" as below.

In the module,
1. Vector travels along the lines
2. A circle/oval represents application of a function to each dimension in a vector
3. A rectangle represents a neural network
4. ùûÇ stands for a logistic activation layer at the top
5. _tanh_ means a hyperbolic tangent activation layer at the top

<img src="https://raw.githubusercontent.com/AlanFermat/AlanFermat.github.io/master/assets/images/lstm4.png">


Note that each module accepts input at a particular time tick as well as state("long term memory") and last output("short term memory"). Then the module will use all of them to output a view of its current state.

A single LSTM unit looks like this,

<img src="https://raw.githubusercontent.com/AlanFermat/AlanFermat.github.io/master/assets/images/lstm5.png">

<div class="side-by-side">
	<div class="toleft">
		<img src="https://raw.githubusercontent.com/AlanFermat/AlanFermat.github.io/master/assets/images/lstm7.png">
	</div>
	<div class="toright">
		<p>First we will run teh "forget gate",</p>
		<ol>
			<li>Last output and new input pass through a neural net with logistic layer at the top</li>
			<li>it will produce all values from 0 to 1</li>
		</ol>
	</div>
</div>


Then we run an "input gate",
1. Input plus last output passes through two NNs
	1. One with a _tanh_ layer at the top producing a value from -1 to 1
	2. One with logistic layer at the top producing a value from 0 to 1
2. Item-by-item multiplication will produce a vector for the current state
	1. The update is added to the long-term memory

In the end, we run "output gate",
1. Push state through a _tanh_ layer at the top producing a value from -1 to 1












