---
title: "TensorFlow - not another introduction II"
layout: post
date: 2019-01-18
image: /assets/images/tensorflow.jpg
headerImage: true
tag:
- Machine Learning
- Neural Network
- Google
- Computation Framework
category: blog
author: Alan Yu
description: An introduction to Google's open-source neural network framework tensorflow

---

## Background 

In this post we will introduce a more interesting, state-of-art neural network model to the readers: Long-short-term-memory Network.

_Long-short-term-memory network_ or _LSTM network_ for short is a special case in Recurrent Neural Network that tries to avoid the "vanishing gradients problem." 

Lost in the jargons? Let's take a step back and try to understand all the words in the above paragraph. <a href="https://en.wikipedia.org/wiki/Recurrent_neural_network">Recurrent Neural Network</a> is a type of neural network that specializes in processing sequences of inputs via its storage of the internal states. 

A typical Recurrent Neural Network looks like the following.

<img src="https://raw.githubusercontent.com/AlanFermat/AlanFermat.github.io/master/assets/images/lstm1.png">

## Vanishing Gradients Problem

If we unroll the network above, we will see something like this,
<img src="https://raw.githubusercontent.com/AlanFermat/AlanFermat.github.io/master/assets/images/lstm6.png">

Recall how we calculate the derivative in the backpropagation from an <a href="https://imalanyu.com/Neural-Network/">early post</a>. 

<div class="side-by-side">
	<div class="toleft">
		<img class="image" src="https://raw.githubusercontent.com/AlanFermat/AlanFermat.github.io/master/assets/images/lstm2.png" alt="Alt Text">
		<figcaption class="caption">Illustration for calculating derivatives in backprop</figcaption>
	</div>
	<div class="toright">
		<p>Note that the derivative of loss with respect to activation function is often less than 1.</p>
		<p>If we have a deep RNN and apply that multiplier constantly as we backprop the errors to the input layer, the derived gradient is approaching zero. </p>
	</div>
</div>

Walla! Here comes the savior -- LSTM network. In LSTM net, we don't push the long term memory through the activation function, i.e. we don't backprop the long term memory of an error all the way to the input layer. Thus, we don't have a vanishing gradients problem!







