---
title: "TensorFlow - not another introduction II"
layout: post
date: 2019-01-18
image: /assets/images/tensorflow.jpg
headerImage: true
tag:
- Machine Learning
- Neural Network
- Google
- Computation Framework
category: blog
author: Alan Yu
description: An introduction to Google's open-source neural network framework tensorflow

---

## Background 

In this post, we will introduce a more interesting, state-of-art neural network model to the readers: Long-short-term-memory Network.

_Long-short-term-memory network_ or _LSTM network_ for short is a special case in Recurrent Neural Network that tries to avoid the "vanishing gradient problem." 

Lost in the jargons? Let's take a step back and try to understand all the words in the above paragraph. <a href="https://en.wikipedia.org/wiki/Recurrent_neural_network">Recurrent Neural Network</a> is a type of neural network that specializes in processing sequences of inputs via its storage of the internal states. 

A typical Recurrent Neural Network looks like the following.

<img src="https://raw.githubusercontent.com/AlanFermat/AlanFermat.github.io/master/assets/images/lstm1.png">

## Vanishing Gradient Problem

If we unroll the network above, we will see something like this,
<img src="https://raw.githubusercontent.com/AlanFermat/AlanFermat.github.io/master/assets/images/lstm6.png">

Recall how we calculate the derivative in the backpropagation from an <a href="https://imalanyu.com/Neural-Network/">early post</a>. 

<div class="side-by-side">
	<div class="toleft">
		<div style="margin-bottom: 100pt">
			<img class="image" src="https://raw.githubusercontent.com/AlanFermat/AlanFermat.github.io/master/assets/images/lstm3.png" alt="Alt Text">
			<figcaption class="caption">Derivative of logistic activation function</figcaption>
		</div>
		<div padding-bottom="10">
			<img class="image" src="https://raw.githubusercontent.com/AlanFermat/AlanFermat.github.io/master/assets/images/lstm2.png" alt="Alt Text">
			<figcaption class="caption">Illustration for calculating derivatives in backprop</figcaption>
		</div>
	</div>
	<div class="toright">
		<p>Note that the derivative of loss with respect to activation function is often less than 1.</p>
		<p>Think about the derivative with respect to the logistic function on the left for example, we know that maximum value is reached when the activation function takes the value of 0.5 which is less than 1.</p>
		<p>If we have a deep RNN and apply that multiplier constantly as we backpropagate the errors to the input layer, the derived gradient is approaching zero. </p>
		<p>The error from the output layer may not be able to influence the upfront layers as they are too far.</p>
		<p>This pattern would make the training stagnant as no correction would be made in the upfront layers due to the errors in the output.</p>
	</div>
</div>

You can see more detailed explanations of the vanishing gradient problem below.

<iframe width="560" height="315" src="https://www.youtube.com/embed/qO_NLVjD6zE" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

## LSTM

### LSTM Module
Walla! Here comes the savior -- LSTM network. In LSTM net, we don't push the long term memory through the activation function, i.e. we don't backpropagate the long term memory of an error all the way to the input layer. Thus, we don't have a vanishing gradient problem!

Let's look at the LSTM "modules" as below.

In the module,
1. Vector travels along the lines
2. A circle/oval represents applications of a function to each dimension in a vector
3. A rectangle represents a neural network
4. ùûÇ stands for a logistic activation layer at the top
5. _tanh_ means a hyperbolic tangent activation layer at the top

<img src="https://raw.githubusercontent.com/AlanFermat/AlanFermat.github.io/master/assets/images/lstm4.png">


Note that each module accepts input at a particular time tick as well as state("long term memory") and last output("short term memory"). Then the module will use all of them to output a view of its current state.

### A Single LSTM Unit

A single LSTM unit looks like this,

<img src="https://raw.githubusercontent.com/AlanFermat/AlanFermat.github.io/master/assets/images/lstm5.png">

<div class="side-by-side">
	<div class="toleft">
		<img src="https://raw.githubusercontent.com/AlanFermat/AlanFermat.github.io/master/assets/images/lstm7.png">
	</div>
	<div class="toright">
		<p>First we will run teh "forget gate",</p>
		<ol>
			<li>Last output and new input pass through a neural net with logistic layer at the top</li>
			<li>it will produce all values from 0 to 1</li>
		</ol>
	</div>
</div>

<div class="side-by-side">
	<div class="toleft">
		<img src="https://raw.githubusercontent.com/AlanFermat/AlanFermat.github.io/master/assets/images/lstm8.png">
	</div>
	<div class="toright">
		<p>Then we run an "input gate",</p>
		<ol>
			<li>Input plus last output passes through two NNs
				<ol>
				<li>One with a tanh layer at the top producing a value from -1 to 1</li>
				<li>One with logistic layer at the top producing a value from 0 to 1</li>
				</ol>
			</li>
			<li>Item-by-item multiplication will produce a vector for the current state
				<ol>
					<li>The update is added to the long-term memory</li>
				</ol>
			</li>
		</ol>
	</div>
</div>

<div class="side-by-side">
	<div class="toleft">
		<img src="https://raw.githubusercontent.com/AlanFermat/AlanFermat.github.io/master/assets/images/lstm9.png">
	</div>
	<div class="toright">
		<p>In the end, we run "output gate",</p>
		<ol>
			<li>Push state through a tanh layer at the top producing a value from -1 to 1</li>
		</ol>
	</div>
</div>


## Code Snippets

With the help of Tensorflow framework, creating an LSTM unit is quite straightforward.

Let's take a brief walk-through.

Set up a 1000-unit LSTM network.

```
hiddenUniots = 1000
lstm = tf.nn.rnn_cell.LSTMCell(hiddenUnits)
``` 

Set up the initial state
```
initialState = lstm.zero_state(batchSize, tf.float32)
```

Train the LSTM net
```
currentState = initialState
for iter in range(maxSeqLen):
    timeTick = input[iter]
    # concatenate the state with the input, 
    # then compute the next state
    (lstm_output, currentState) = lstm(timeTick, currentState)
```

where _input_ is your stacked input variables.

Note that _lstm_output_ is your final output, you can do a feedforward or softmax afterward to regularize it.

And..., that's it. 

Within five lines you could set up an LSTM module and you can choose optimizer to train the model(how to do that? check out the <a href="https://imalanyu.com/TensorFlow/">TensorFlow Tutorial Post</a>)


## More About LSTM

If you wanna know more about LSTM, check out the following
<ol>
	<li><a href="https://arxiv.org/pdf/1503.04069.pdf">Original Paper</a></li>
	<li><a href="https://www.youtube.com/watch?v=WCUNPb-5EYI">Video Tutorial</a></li>
</ol>







