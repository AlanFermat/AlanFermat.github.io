<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.7.3">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2018-12-05T20:08:39-06:00</updated><id>http://localhost:4000/</id><title type="html">Alan Yu</title><subtitle>A blog about technology and stuff related</subtitle><entry><title type="html">TensorFlow - yet another introduction</title><link href="http://localhost:4000/TensorFlow/" rel="alternate" type="text/html" title="TensorFlow - yet another introduction" /><published>2018-12-04T00:00:00-06:00</published><updated>2018-12-04T00:00:00-06:00</updated><id>http://localhost:4000/TensorFlow</id><content type="html" xml:base="http://localhost:4000/TensorFlow/">&lt;h2 id=&quot;background&quot;&gt;Background&lt;/h2&gt;

&lt;p&gt;After reading my blog about neural networks, some readers had questions on how to avoid implementing the neural network from the scratch. They found it very hard to do backpropagations by manually taking derivatives for each processing element at each layer in the neural network. The computation is time-consuming and could sometimes be totally messed up if you make a single mistake.&lt;/p&gt;

&lt;p&gt;Therefore, today I wanna introduce this amazing neural network framework by Google to you. As many of you may have already heard of it, this open-source framework is called TensorFlow.&lt;/p&gt;</content><author><name>Alan Yu</name></author><category term="blog" /><category term="Machine Learning" /><category term="Neural Network" /><category term="Google" /><category term="Computation Framework" /><summary type="html">Background</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/images/nnTitle.jpg" /></entry><entry><title type="html">Twitter Higher and Lower game</title><link href="http://localhost:4000/twitter-game/" rel="alternate" type="text/html" title="Twitter Higher and Lower game" /><published>2018-04-01T00:00:00-05:00</published><updated>2018-04-01T00:00:00-05:00</updated><id>http://localhost:4000/twitter-game</id><content type="html" xml:base="http://localhost:4000/twitter-game/">&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/AlanFermat/twitterhigherlower/master/ingame.png&quot; alt=&quot;Screenshot&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;twitter-lower-and-higher&quot;&gt;Twitter Lower and Higher&lt;/h2&gt;
&lt;p&gt;This is a web game developed in 24 hours during Rice Hackathon that will allow users to guess which person has a higher Twitter followers.&lt;/p&gt;

&lt;p&gt;In each round, you will be given two celebrities or famous organizations. You need to guess which one of the two has a higher followers on twitter.&lt;/p&gt;

&lt;p&gt;If your answer is correct, you can go to the next round; otherwise, the game is over and you will receive your scores.&lt;/p&gt;

&lt;p&gt;Try to reach a higher score!&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/AlanFermat/twitterhigherlower&quot;&gt;Check it out&lt;/a&gt; here.
If you need some help, just &lt;a href=&quot;https://github.com/AlanFermat/twitterhigherlower/issues&quot;&gt;tell me&lt;/a&gt;.&lt;/p&gt;</content><author><name>Alan Yu</name></author><category term="project" /><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://raw.githubusercontent.com/AlanFermat/twitterhigherlower/master/twitter.png" /></entry><entry><title type="html">Self-organizing Map – unsupervised neural network</title><link href="http://localhost:4000/SOM/" rel="alternate" type="text/html" title="Self-organizing Map -- unsupervised neural network" /><published>2018-03-21T22:44:00-05:00</published><updated>2018-03-21T22:44:00-05:00</updated><id>http://localhost:4000/SOM</id><content type="html" xml:base="http://localhost:4000/SOM/">&lt;h2 id=&quot;what-is-unsupervised-learning&quot;&gt;What is unsupervised learning?&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Unsupervised_learning&quot;&gt;Unsupervised machine learning&lt;/a&gt; is the machine learning task of inferring a function that describes the structure of “unlabeled” data. The goal for unsupervised learning is to detect some pattern behind a mass of data without any priori knowledge about the data. Sounds like roman policier? That’s right!&lt;/p&gt;

&lt;p&gt;Since the beginning of this field, a lot of unsupervised learning algorithms has been developed and applied widely. Algorithms like K-Means, hierarchical clustering, and principal component analysis are crucial tools in industry and academia.&lt;/p&gt;

&lt;p&gt;Here is a useful &lt;a href=&quot;https://medium.com/machine-learning-for-humans/unsupervised-learning-f45587588294&quot;&gt;link&lt;/a&gt; that explains the concepts of unsupervised learning in more detail.&lt;/p&gt;

&lt;h2 id=&quot;what-is-self-organizing-map&quot;&gt;What is self-organizing map?&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Self-organizing_map&quot;&gt;Self-organizing map (SOM)
&lt;/a&gt; is an amazing neural network. The number of published research papers related to self-organizing map ranked the first place from 2010 to 2015.&lt;/p&gt;

&lt;p&gt;In this neural network, the output layer is a grid in which each neuron represents a value that tries to approximate the corresponding input value. The interesting part is that each neuron is actually a weight that needs to be tuned throughout the training process. The goal for this neuron network is to learn representation of all input data points on neurons of the output grid and then cluster the data via similarities between neurons.&lt;/p&gt;

&lt;div class=&quot;side-by-side&quot;&gt;
	&lt;div class=&quot;toleft&quot;&gt;
		&lt;img class=&quot;image&quot; src=&quot;https://github.com/AlanFermat/AlanFermat.github.io/blob/master/assets/images/som2.jpg?raw=true&quot; alt=&quot;Alt Text&quot; /&gt;
		&lt;figcaption class=&quot;caption&quot;&gt;Illustrion of SOM&lt;/figcaption&gt;
	&lt;/div&gt;
	&lt;div class=&quot;toright&quot;&gt;
		&lt;p&gt;The major paramter is the number of neurons in the grid, called prototypes. The initialization of weights, learning rates and stopping criteria are the same as those of other neural networks.&lt;/p&gt; 
		&lt;p&gt;As we can see from the left, each data point is mapped to a neuron that best approximates the value of that data point. Notice that the learning process is to adjust weights so that we minimize the &quot;distance&quot; between the input data point and the neuron that represents it. &lt;/p&gt;
	&lt;/div&gt;
	&lt;div class=&quot;toleft&quot;&gt;
		&lt;img class=&quot;image&quot; src=&quot;https://github.com/AlanFermat/AlanFermat.github.io/blob/master/assets/images/som3.jpg?raw=true&quot; alt=&quot;Alt Text&quot; /&gt;
		&lt;figcaption class=&quot;caption&quot;&gt;Topological preserving&lt;/figcaption&gt;
	&lt;/div&gt;
	&lt;div class=&quot;toright&quot;&gt;
		&lt;p&gt; The most magic part about self-organizing map is that it is topologically preserved meaning that the neighbors of the original data point *A* will be mapped to the neighbors of *A's* representation in the output grid.&lt;/p&gt;
		&lt;p&gt; This property makes self-organizing map unique among all clustering algorithms. Since topological preservation can help the neural network recognize the patterns easily when the number of clusters are huge, self-organizing map is especially good at recognizing terrains.&lt;/p&gt;
	&lt;/div&gt;
	&lt;div class=&quot;toleft&quot;&gt;
		&lt;img class=&quot;image&quot; src=&quot;https://github.com/AlanFermat/AlanFermat.github.io/blob/master/assets/images/som4.jpg?raw=true&quot; alt=&quot;Alt Text&quot; /&gt;
		&lt;figcaption class=&quot;caption&quot;&gt;Mathematical formula&lt;/figcaption&gt;
	&lt;/div&gt;
	&lt;div class=&quot;toright&quot;&gt;
		&lt;p&gt; Here comes the mathematical formula for SOM. As we can see, one round of training consists of two steps. The first one is competition in which we will choose a prototype closest to the given pattern based on the information we have. At the second step, we update our weight based on the a predefined distance of our prototype from the original pattern. &lt;/p&gt;
		&lt;p&gt;Note that the *h* function that is used in the updating is analogous to gradient descent in artificial neural networks. However, normally we will use Manhattan distance here instead of Euclidean to preserve the topological structure. &lt;/p&gt;
	&lt;/div&gt;

&lt;/div&gt;

&lt;p&gt;After the above explanation, I know some of you are still unclear about how it works or how the mathematical formula works, so do not hesitate to check out the following links:&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt; &lt;a href=&quot;http://www.pitt.edu/~is2470pb/Spring05/FinalProjects/Group1a/tutorial/som.html&quot;&gt;Lecture notes&lt;/a&gt;&lt;/li&gt;
	&lt;li&gt; &lt;a href=&quot;https://www.youtube.com/watch?v=LjJeT7rwvF4&quot;&gt;Lecture video (unrelated to the above link)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;can-you-show-me-an-example&quot;&gt;Can you show me an example?&lt;/h2&gt;

&lt;p&gt;First we generate four sets of 3-dimensional random Gaussian vectors with 1000 points in each set, each with a variance of &lt;strong&gt;0.1&lt;/strong&gt;. Then we center the data sets at &lt;strong&gt;(0,0,0), (0,7,0), (7,0,0), and (7,7,0)&lt;/strong&gt;, respectively.&lt;/p&gt;

&lt;p&gt;We will train a SOM with 100 prototypes trying to cluster the data into 4 clusters.&lt;/p&gt;

&lt;p&gt;Source codes are uploaded &lt;a href=&quot;afs&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;div class=&quot;side-by-side&quot;&gt;
	&lt;div class=&quot;toleft&quot;&gt;
		&lt;img class=&quot;image&quot; src=&quot;https://github.com/AlanFermat/AlanFermat.github.io/blob/master/assets/images/som5.jpg?raw=true&quot; alt=&quot;Alt Text&quot; /&gt;
		&lt;figcaption class=&quot;caption&quot;&gt;Neural network parameters&lt;/figcaption&gt;
	&lt;/div&gt;
	&lt;div class=&quot;toright&quot;&gt;
		&lt;p&gt; We will use the parameters as suggested by the left to train and test the data. &lt;/p&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;side-by-side&quot;&gt;
	&lt;div class=&quot;toleft&quot;&gt;
		&lt;img class=&quot;image&quot; src=&quot;https://github.com/AlanFermat/AlanFermat.github.io/blob/master/assets/images/som6.jpg?raw=true&quot; alt=&quot;Alt Text&quot; /&gt;
		&lt;figcaption class=&quot;caption&quot;&gt;visualization of the grid after 250k training steps&lt;/figcaption&gt;
	&lt;/div&gt;
	&lt;div class=&quot;torights&quot;&gt;
		&lt;p&gt; Note that in this graph, blue dots are generated points that are to be clustered. Red dots represent prototypes For illustration, we connect them by black solid lines. &lt;/p&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;From the result, we can see that most prototypes are bundled and gathered to four corners. Because there are no cluster in the middle, the grid is stretched out tightly making sure maximum of prototypes can match the data points optimally.&lt;/p&gt;

&lt;p&gt;Source code is &lt;a href=&quot;https://github.com/AlanFermat/Blogs/blob/master/SOM/SOM.m&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;reference&quot;&gt;Reference&lt;/h2&gt;

&lt;p&gt; http://www.cs.nott.ac.uk/~pszqiu/Teaching/2004/G5BNEC/G5BNEC-SOM.pdf &lt;/p&gt;
&lt;p&gt;https://link.springer.com/referenceworkentry/10.1007%2F978-3-540-92910-9_19&lt;/p&gt;
&lt;p&gt;https://www.sciencedirect.com/science/article/pii/S0925231298000307&lt;/p&gt;</content><author><name>Alan Yu</name></author><category term="blog" /><category term="Machine Learning" /><category term="Neural Network" /><category term="Self-organizing Map" /><category term="Unsupervised Learning" /><summary type="html">What is unsupervised learning?</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/images/som1.jpg" /></entry><entry><title type="html">Shelper</title><link href="http://localhost:4000/Shelper/" rel="alternate" type="text/html" title="Shelper" /><published>2017-09-22T00:00:00-05:00</published><updated>2017-09-22T00:00:00-05:00</updated><id>http://localhost:4000/Shelper</id><content type="html" xml:base="http://localhost:4000/Shelper/">&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/AlanFermat/Shelper/master/frontend/shelper.jpg&quot; alt=&quot;Screenshot&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;shelper&quot;&gt;Shelper&lt;/h2&gt;

&lt;p&gt;This is an API built by 4 members within 48 hours during Rice Hackathon.&lt;/p&gt;

&lt;p&gt;This API connects emergency shelters with people who are looking for help during Hurricane Harvey.&lt;/p&gt;

&lt;p&gt;I implemented customized Google Map API (JSON and react.js) and visualized the live data on the screen from database (MongoDB). We deployed this API as a web app via Flask.&lt;/p&gt;

&lt;p&gt;A user can use this app to login, either seeking for active shelters or donating necessities to the nearby shelters.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/AlanFermat/Shelper&quot;&gt;Check it out&lt;/a&gt; here.
If you need some help, just &lt;a href=&quot;https://github.com/AlanFermat/Shelper/issues&quot;&gt;tell me&lt;/a&gt;.&lt;/p&gt;</content><author><name>Alan Yu</name></author><category term="project" /><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://raw.githubusercontent.com/AlanFermat/Shelper/master/logo.png" /></entry><entry><title type="html">Neural Network – an introduction to function approximation</title><link href="http://localhost:4000/Neural-Network/" rel="alternate" type="text/html" title="Neural Network -- an introduction to function approximation" /><published>2017-07-22T00:00:00-05:00</published><updated>2017-07-22T00:00:00-05:00</updated><id>http://localhost:4000/Neural-Network</id><content type="html" xml:base="http://localhost:4000/Neural-Network/">&lt;h2 id=&quot;what-is-neural-network&quot;&gt;What is neural network?&lt;/h2&gt;

&lt;h3 id=&quot;background&quot;&gt;Background&lt;/h3&gt;

&lt;p&gt;As computer sciecne students, we often heard these fancy stuffs in life: image classification, pattern recognition, convolutional neural network, machine learning, etc. Sometimes, we are so overwhelmed by the jargons in the field that we do not wanna explore the field ourselves.&lt;/p&gt;

&lt;p&gt;However, we are all human beings living in the 21st century – &lt;a href=&quot;https://www.tandfonline.com/doi/full/10.1080/20964471.2017.1397411&quot;&gt; the era of big data &lt;/a&gt;, making techniques such as machine learning useful and meaningful for data analysis.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Machine_learning&quot;&gt;Machine learning&lt;/a&gt; is a century-old field that recently became tremendously popular due to the demand for convenient tools facilitates data analysis. It has two intersecting subfields based on techniques, namely statistical machine lerning and neural network. Whereas statistical machine learning involves a heavy load of statistics, neural networks stress more on designing and parameter-tuning.&lt;/p&gt;

&lt;p&gt;Based on the data we have, we can also divide machine learning into &lt;a href=&quot;https://en.wikipedia.org/wiki/Supervised_learning&quot;&gt;supervised&lt;/a&gt; and &lt;a href=&quot;https://en.wikipedia.org/wiki/Unsupervised_learning&quot;&gt;unsupervised learning&lt;/a&gt;. While supervised learning is to figure out the pattern with the guidance of a given standard, unsupervised learning is to recognize the pattern without any additional information.&lt;/p&gt;

&lt;p&gt;Neural network is one group of algorithms used for machine learning that models the data using graphs of artificial neurons. It tries to mimic how the neurons in human brains work.&lt;/p&gt;

&lt;p&gt;In this post, we will focus majorly on applying neural network to function approximation. Since we will be informed of what our outputs should look like, we will be discussing about supervised neural network techniques only. Moreover, for the sake of illustration we only focus on Artificial Neural Network architecture.&lt;/p&gt;

&lt;p&gt;For those who are interested in Long-short Term Memory Neural Network, Convolutional Neural Network and Generative Adversarial Network, please check out the links below:&lt;/p&gt;

&lt;ul&gt; 
	&lt;li&gt; Long-short Term Memory: 
	&lt;a href=&quot;http://colah.github.io/posts/2015-08-Understanding-LSTMs/&quot;&gt;LSTM Blog&lt;/a&gt;
	&lt;a href=&quot;http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture10.pdf&quot;&gt;Stanford CS231n&lt;/a&gt;
	&lt;/li&gt;
	&lt;li&gt; Convolutional: 
	&lt;a href=&quot;https://www.google.com/search?q=convolutional+neural+network&amp;amp;rlz=1C5CHFA_enUS708US708&amp;amp;oq=convolutional&amp;amp;aqs=chrome.0.0j69i57j69i65j69i60j0l2.1802j1j9&amp;amp;sourceid=chrome&amp;amp;ie=UTF-8&quot;&gt; CNN Blog &lt;/a&gt;
	&lt;a href=&quot;http://cs231n.github.io/convolutional-networks/&quot;&gt;Stanford CS231n&lt;/a&gt;
	&lt;/li&gt;
	&lt;li&gt; Generative Adversarial: 
	&lt;a href=&quot;https://medium.com/@jonathan_hui/gan-why-it-is-so-hard-to-train-generative-advisory-networks-819a86b3750b&quot;&gt; GAN Blog&lt;/a&gt;
	&lt;a href=&quot;https://arxiv.org/abs/1406.2661&quot;&gt;Original Paper&lt;/a&gt;
	&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;technical-detail&quot;&gt;Technical detail&lt;/h3&gt;

&lt;h4 id=&quot;1-components&quot;&gt;1. Components&lt;/h4&gt;

&lt;div class=&quot;side-by-side&quot;&gt;
	&lt;div class=&quot;toleft&quot;&gt;
		&lt;img class=&quot;image&quot; src=&quot;https://github.com/AlanFermat/AlanFermat.github.io/blob/master/assets/images/nn1.jpg?raw=true&quot; alt=&quot;Alt Text&quot; /&gt;
		&lt;figcaption class=&quot;caption&quot;&gt;Illustration for one layer&lt;/figcaption&gt;
	&lt;/div&gt;
	&lt;div class=&quot;toright&quot;&gt;
		&lt;p&gt; A neuron network is a layer-by-layer structure. At each layer, it consists of processing elements (referred as PEs afterwards) and transfer functions. &lt;/p&gt;
		&lt;p&gt; Usually, the first layer of a network is called input layer, the last layer is called output layer and the layers in between are hidden layers. &lt;/p&gt;
		&lt;p&gt; The architecture of a neuron network is composed of the way that these layers combine together. For example, a network with 3-3-1 is a neural network where the first and second layer consist of 3 PEs and the output layer is of 1 PE. Note that the neural network on the left is of the topology N-1 with only two layers, one input layer and one output layer. &lt;/p&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;Useful video explaning neural networks in more details:&lt;/p&gt;
&lt;iframe width=&quot;699&quot; height=&quot;393&quot; src=&quot;https://www.youtube.com/embed/aircAruvnKk&quot; frameborder=&quot;0&quot; allow=&quot;autoplay; encrypted-media&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;hr /&gt;

&lt;h4 id=&quot;2-forward-propagation&quot;&gt;2. Forward propagation&lt;/h4&gt;

&lt;div class=&quot;side-by-side&quot;&gt;
	&lt;div class=&quot;toleft&quot;&gt;
		&lt;img class=&quot;image&quot; src=&quot;https://github.com/AlanFermat/AlanFermat.github.io/blob/master/assets/images/nn2.jpg?raw=true&quot; alt=&quot;Alt Text&quot; /&gt;
		&lt;figcaption class=&quot;caption&quot;&gt;what does weight look like&lt;/figcaption&gt;
	&lt;/div&gt;
	&lt;div class=&quot;toright&quot;&gt;
		&lt;p&gt; Weight is a mathematical representation of how important a factor is in the neural network. Assuming the identity transfer function, the higher the value of weight is, the more effect of that weight will be taken into account when calculating the output of the current layer. &lt;/p&gt;
		&lt;p&gt; The weight in the form of the left is the weight of the first layer from jth PE to the ith PE in the next layer.&lt;/p&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;side-by-side&quot;&gt;
	&lt;div class=&quot;toleft&quot;&gt;
		&lt;img class=&quot;image&quot; src=&quot;https://github.com/AlanFermat/AlanFermat.github.io/blob/master/assets/images/nn3.jpg?raw=true&quot; alt=&quot;Alt Text&quot; /&gt;
		&lt;figcaption class=&quot;caption&quot;&gt;transfer function&lt;/figcaption&gt;
	&lt;/div&gt;
	&lt;div class=&quot;toright&quot;&gt;
		&lt;p&gt; An transfer function a node defines the output of that node given an input or set of inputs. &lt;/p&gt;
		&lt;p&gt; The simplest transfer function is identity function which gives an identity map from the output of the previous layer to the input of the next layer. Researchers and scientists normally choose sigmoid function or tanh function as transfer functions. However, the choice of the transfer function is open to discuss and their relative advantages are discussed &lt;a href=&quot;https://papers.nips.cc/paper/874-how-to-choose-an-transfer-function.pdf&quot;&gt;here&lt;/a&gt;. &lt;/p&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;In forward propagation, we will first aggregate the results calculated from the previous layer, applying transfer function as indicated above. We will do this layer by layer toward the output layer.&lt;/p&gt;

&lt;div class=&quot;side-by-side&quot;&gt;
	&lt;div class=&quot;toleft&quot;&gt;
		&lt;img class=&quot;image&quot; src=&quot;https://github.com/AlanFermat/AlanFermat.github.io/blob/master/assets/images/nn4.jpg?raw=true&quot; alt=&quot;Alt Text&quot; /&gt;
		&lt;figcaption class=&quot;caption&quot;&gt;Mathematical formula for forward propagation&lt;/figcaption&gt;
	&lt;/div&gt;
	&lt;div class=&quot;toright&quot;&gt;
		&lt;p&gt; The mathematic formula for forward propagation is shown on the left. You can iteratively do forward propagations for all layers. &lt;/p&gt;
		&lt;p&gt;We can also rewrite the summation part by matrix multiplication. We will leave this as an exercise for our readers. &lt;/p&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;Useful video for further illustration:&lt;/p&gt;
&lt;iframe width=&quot;699&quot; height=&quot;393&quot; src=&quot;https://www.youtube.com/embed/UJwK6jAStmg&quot; frameborder=&quot;0&quot; allow=&quot;autoplay; encrypted-media&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;h4 id=&quot;3-backward-propogation&quot;&gt;3. Backward propogation&lt;/h4&gt;

&lt;p&gt;When we reach the output layer in our neural network, we want to see how good our predicted result is compared to the desired output. So we will designate the network with an error function (usually Mean Square Error or Edit Distance) to evaluate how well we are doing with our current weights values and structures.&lt;/p&gt;

&lt;div class=&quot;side-by-side&quot;&gt;
	&lt;div class=&quot;toleft&quot;&gt;
		&lt;img class=&quot;image&quot; src=&quot;https://github.com/AlanFermat/AlanFermat.github.io/blob/master/assets/images/nn5.jpg?raw=true&quot; alt=&quot;Alt Text&quot; /&gt;
		&lt;figcaption class=&quot;caption&quot;&gt;Mathematical formula for backward propagation&lt;/figcaption&gt;
	&lt;/div&gt;
	&lt;div class=&quot;toright&quot;&gt;
		&lt;p&gt; We start from the output layer, comparing the desired results with predicted results then tracing back one layer at a time. &lt;/p&gt;
		&lt;p&gt; The adjustments are made to our weights through various methods where gradient descent is the most popular one. &lt;/p&gt;
		&lt;p&gt; Once we are done adjusting weights and reach the input layer, we will redo the forward propagation again. &lt;/p&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;Useful video for further illustration:&lt;/p&gt;
&lt;iframe width=&quot;640&quot; height=&quot;360&quot; src=&quot;https://www.youtube.com/embed/GlcnxUlrtek&quot; frameborder=&quot;0&quot; allow=&quot;autoplay; encrypted-media&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;h4 id=&quot;4-stopping-criteria&quot;&gt;4. Stopping criteria&lt;/h4&gt;

&lt;p&gt;Before we start training our neural network, we will pre-define a stopping criteria for our network. For example, we can say if the error is within 10e-5 then we will stop the training or if we have gone 10e5 iterations then we stop training further.&lt;/p&gt;

&lt;p&gt;Stopping criteria is crucial since we need to set a goal for our network to reach. There are various ways to determine where to stop. Check the links below:&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;&lt;a href=&quot;https://www.ibm.com/support/knowledgecenter/en/SS3RA7_15.0.0/com.ibm.spss.modeler.help/idh_neuralnet_stopping_rules.htm&quot;&gt;IBM blog&lt;/a&gt;&lt;/li&gt;
	&lt;li&gt;&lt;a href=&quot;http://image.diku.dk/shark/sphinx_pages/build/html/rest_sources/tutorials/first_steps/when_to_stop.html&quot;&gt;Shark developer post&lt;/a&gt;&lt;/li&gt;
	&lt;li&gt;&lt;a href=&quot;https://www.researchgate.net/post/How_do_I_know_when_to_stop_training_a_neural_network&quot;&gt;Research Gate Discussion Forum&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;5-implementation&quot;&gt;5. Implementation&lt;/h4&gt;

&lt;p&gt;I have a ready-to-go 3-layer neural network implemented in Matlab for you &lt;a href=&quot;https://github.com/AlanFermat/2R-robotic-arm-with-neural-network/blob/master/neuralNetwork.m&quot;&gt;here&lt;/a&gt;. It can be easily translated to Python using TensorFlow or pyTorch. You can also build your own neural networks.&lt;/p&gt;

&lt;ul&gt; 
	&lt;li&gt;A tutorial for TensorFlow users: &lt;a href=&quot;http://web.stanford.edu/class/cs20si/&quot;&gt; Stanford CS 20SI&lt;/a&gt;&lt;/li&gt;
	&lt;li&gt;Implement an ANN using pyTorch: &lt;a href=&quot;https://github.com/AlanFermat/Blogs/tree/master/ANNPytorch&quot;&gt; PyTorch Implementation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;what-is-function-approximation&quot;&gt;What is function approximation?&lt;/h2&gt;

&lt;p&gt;In general, a function approximation problem asks us to select a function among a well-defined class[clarification needed] that closely matches (“approximates”) a target function in a task-specific way.&lt;/p&gt;

&lt;p&gt;By &lt;a href=&quot;http://math.uchicago.edu/~may/REU2016/REUPapers/Gaddy.pdf&quot;&gt;Stone–Weierstrass theorem &lt;/a&gt;, every continuous function defined on a closed interval can be uniformly approximated as closely as desired by a polynomial function. We know from above that, if we choose linear function as our transfer function, then at each iteration of neural network, we are doing a matrix multiplication at each step.&lt;/p&gt;

&lt;p&gt;If we expand matrix multiplication, it is easy to see that the whole process of forward propogation is equivalent to using a polynomial to approximate the target function. By Stone-Weierstrass theorem, we have that neural network has the ability to approximate functions satisfying specific requirements (some constraints including continuity, domain, etc.).&lt;/p&gt;

&lt;p&gt;There is a formal theorem supporting this! The &lt;a href=&quot;https://en.wikipedia.org/wiki/Universal_approximation_theorem&quot;&gt;universal approximation theorem &lt;/a&gt; states that a feed-forward network with a single hidden layer containing a finite number of neurons can approximate continuous functions on compact subsets of n-dimensional Euclidean space, under mild assumptions on the transfer function. If you are interested in this theorem, you probably wanna read the following:&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;&lt;a href=&quot;https://blog.goodaudience.com/neural-networks-part-1-a-simple-proof-of-the-universal-approximation-theorem-b7864964dbd3&quot;&gt;blog&lt;/a&gt;&lt;/li&gt;
	&lt;li&gt;&lt;a href=&quot;http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.101.2647&amp;amp;rep=rep1&amp;amp;type=pdf&quot;&gt;Paper&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;can-you-show-me-an-example&quot;&gt;Can you show me an example?&lt;/h2&gt;

&lt;p&gt;We will illustrate the application of a simple 3-layer aritificial neural network in approximating inverse function &lt;strong&gt;f(x) = 1/x&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Source codes are uploaded &lt;a href=&quot;https://github.com/AlanFermat/Blogs/tree/master/OneOverX&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;div class=&quot;side-by-side&quot;&gt;
	&lt;div class=&quot;toleft&quot;&gt;
		&lt;img class=&quot;image&quot; src=&quot;https://github.com/AlanFermat/AlanFermat.github.io/blob/master/assets/images/nn6.jpg?raw=true&quot; alt=&quot;Alt Text&quot; /&gt;
		&lt;figcaption class=&quot;caption&quot;&gt;Neural network parameters&lt;/figcaption&gt;
	&lt;/div&gt;
	&lt;div class=&quot;toright&quot;&gt;
		&lt;p&gt; We will use the parameters as suggested by the left to train and test the data. &lt;/p&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;side-by-side&quot;&gt;
	&lt;div class=&quot;toleft&quot;&gt;
		&lt;img class=&quot;image&quot; src=&quot;https://github.com/AlanFermat/AlanFermat.github.io/blob/master/assets/images/nn7.jpg?raw=true&quot; alt=&quot;Alt Text&quot; /&gt;
		&lt;figcaption class=&quot;caption&quot;&gt;Error plot&lt;/figcaption&gt;
	&lt;/div&gt;
	&lt;div class=&quot;toright&quot;&gt;
		&lt;img class=&quot;image&quot; src=&quot;https://github.com/AlanFermat/AlanFermat.github.io/blob/master/assets/images/nn8.jpg?raw=true&quot; alt=&quot;Alt Text&quot; /&gt;
		&lt;figcaption class=&quot;caption&quot;&gt;Predicted VS Desired&lt;/figcaption&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;h2 id=&quot;what-are-other-applications&quot;&gt;What are other applications?&lt;/h2&gt;

&lt;p&gt;We also use a 3-layer neural network to teach a robotic arm how to draw certain pictures. The result is quite fun!&lt;/p&gt;

&lt;p&gt;Check this out: 
&lt;a href=&quot;https://github.com/AlanFermat/2R-robotic-arm-with-neural-network&quot;&gt; ANN application in Robotics &lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;reference&quot;&gt;Reference&lt;/h2&gt;
&lt;p&gt;https://www.tandfonline.com/doi/full/10.1080/20964471.2017.1397411&lt;/p&gt;

&lt;p&gt;http://math.uchicago.edu/~may/REU2016/REUPapers/Gaddy.pdf&lt;/p&gt;</content><author><name>Alan Yu</name></author><category term="blog" /><category term="Machine Learning" /><category term="Neural Network" /><category term="Function Approximation" /><category term="Supervised Learning" /><summary type="html">What is neural network?</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/images/nnTitle.jpg" /></entry><entry><title type="html">Text mining for finding the best place pursuing medical research</title><link href="http://localhost:4000/text-mining-for-medical-research/" rel="alternate" type="text/html" title="Text mining for finding the best place pursuing medical research" /><published>2016-04-11T00:00:00-05:00</published><updated>2016-04-11T00:00:00-05:00</updated><id>http://localhost:4000/text-mining-for-medical-research</id><content type="html" xml:base="http://localhost:4000/text-mining-for-medical-research/">&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/AlanFermat/Text-mining-for-medical-research/master/wordcloud.jpg&quot; alt=&quot;Screenshot&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;text-mining-for-finding-the-best-place-pursuing-medical-research&quot;&gt;Text mining for finding the best place pursuing medical research&lt;/h2&gt;

&lt;h3 id=&quot;where-is-the-best-place-for-medical-research&quot;&gt;Where is the best place for medical research?&lt;/h3&gt;

&lt;p&gt;We explore where the best places for health research and innovation are, and how this relates to the rates of specific illnesses.&lt;/p&gt;

&lt;p&gt;“Where” is meant to be general and refers not only to specific locations, such as cities/states, but also specific diseases, such as obesity, diabetes, and other characteristics. We then rank the 11 top cities for health research and innovation.&lt;/p&gt;

&lt;p&gt;In order to figure out where the best places are, we first analyze funding on three dimensions: type of disease, time, and location.&lt;/p&gt;

&lt;p&gt;Based on this high-level analysis, we find location is the most important factor. We deep dive into location, specifically city-level analysis.&lt;/p&gt;

&lt;p&gt;We breakdown the importance into three categories, which we call: access to talent, access to resources, and access to funding. Based on these three dimensions, we rank the top 11 cities. Based on our ranking system. New York is the top city, while Seattle is the bottom among the 11 cities.&lt;/p&gt;

&lt;p&gt;The results are presented beautifully via ggplot and wordcloud using R!&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/AlanFermat/Text-mining-for-medical-research&quot;&gt;Check it out&lt;/a&gt; here.
If you need some help, just &lt;a href=&quot;https://github.com/AlanFermat/Text-mining-for-medical-research/issues&quot;&gt;tell me&lt;/a&gt;.&lt;/p&gt;</content><author><name>Alan Yu</name></author><category term="project" /><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://raw.githubusercontent.com/AlanFermat/Text-mining-for-medical-research/master/data-mining.jpg" /></entry></feed>