<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.7.3">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2019-10-25T10:57:11-05:00</updated><id>http://localhost:4000/</id><title type="html">Alan Yu</title><subtitle>A blog about technology and stuff related</subtitle><entry><title type="html">A discussion on plant-based diets</title><link href="http://localhost:4000/Plant-Based-Diet/" rel="alternate" type="text/html" title="A discussion on plant-based diets" /><published>2019-10-23T22:44:00-05:00</published><updated>2019-10-23T22:44:00-05:00</updated><id>http://localhost:4000/Plant-Based-Diet</id><content type="html" xml:base="http://localhost:4000/Plant-Based-Diet/">&lt;p&gt;A plant-based diet is a diet consisting mostly or entirely of foods derived from plants, including vegetables, grains, nuts, seeds, legumes and fruits, and with few or no animal products. A plant-based diet is not necessarily vegetarian. The use of the phrase “plant-based” changed over time. However, in this article we will refer to a “plant-based” diet as the definition we gave above.&lt;/p&gt;

&lt;p&gt;Netflix recently released &lt;em&gt;The Game Changers&lt;/em&gt;, a documentary directed by Academy Award winner Louie Psihoyos (The Cove) trying to demolish myths against plant-based diets and reveal the antique notion of the meats among athletes.&lt;/p&gt;

&lt;p&gt;According to the documentary, many athletes have this archaic notion that meats give the people energy. I believe a lot of people still do these days. When some men see a big dude eating a huge steak in a restaurant, they would say, “I wanna be like him.” When they see a dude eating salad, however, they would think that he is soft or not “man” enough. I found it very interesting that people automatically associate eating meat with being mascular even though the former not necessarily facilitates the latter.&lt;/p&gt;

&lt;p&gt;Based on a paper &lt;em&gt;Blood rheology in vegetarians&lt;/em&gt; (E. Ernst, et al, 1986), a plant-based diet is low in saturated fat and free of cholesterolcan. A plant-based diet can decrease human’s blood viscosity allowing more oxygen can reach the muscles. With more oxygen in the muscles, it is very easy for the athletes to improve the athletic performance.&lt;/p&gt;</content><author><name>Alan Yu</name></author><category term="blog" /><category term="Nutrition" /><category term="Diet" /><category term="Plant-based" /><summary type="html">A plant-based diet is a diet consisting mostly or entirely of foods derived from plants, including vegetables, grains, nuts, seeds, legumes and fruits, and with few or no animal products. A plant-based diet is not necessarily vegetarian. The use of the phrase “plant-based” changed over time. However, in this article we will refer to a “plant-based” diet as the definition we gave above.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/images/plant1.jpg" /></entry><entry><title type="html">Music List</title><link href="http://localhost:4000/music-list/" rel="alternate" type="text/html" title="Music List" /><published>2019-08-16T00:00:00-05:00</published><updated>2019-08-16T00:00:00-05:00</updated><id>http://localhost:4000/music-list</id><content type="html" xml:base="http://localhost:4000/music-list/">&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/AlanFermat/musicList/master/demo_video_preview.png&quot; alt=&quot;Screenshot&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;music-list&quot;&gt;Music List&lt;/h2&gt;

&lt;p&gt;This is a side project I made in leisure time for fun.&lt;/p&gt;

&lt;p&gt;This is a customized music list that will allow the users to listen to customized songs from YouTube channels.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Check it out &lt;a href=&quot;https://github.com/AlanFermat/musicList&quot;&gt;here&lt;/a&gt;.
If you need some help, just &lt;a href=&quot;https://github.com/AlanFermat/musicList/issues&quot;&gt;tell me&lt;/a&gt;.&lt;/p&gt;</content><author><name>Alan Yu</name></author><category term="project" /><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://raw.githubusercontent.com/AlanFermat/musicList/master/youtube.png" /></entry><entry><title type="html">My internship @Facebook Summer 2019</title><link href="http://localhost:4000/FB-Intern-Experience/" rel="alternate" type="text/html" title="My internship @Facebook Summer 2019" /><published>2019-08-15T00:00:00-05:00</published><updated>2019-08-15T00:00:00-05:00</updated><id>http://localhost:4000/FB-Intern-Experience</id><content type="html" xml:base="http://localhost:4000/FB-Intern-Experience/">&lt;p&gt;During my internship at Facebook in summer 2019, I learned a lot about engineering in real-world settings. I was assigned to the Crisis Response team. The core product the team works on is the Crisis Response Page. When natural disasters occur(for example, an earthquake), the users can use the page to know if their friends are marked themselves safe and if other people in the affected area are requesting or offering help.&lt;/p&gt;

&lt;p&gt;My main projects are on the FBLite interface. FBLite is an interface that generally targets the lower-end devices with less battery and processing power. A large portion of the users of FBLite is from developing countries. I built two main projects on FBLite for the team during the summer.&lt;/p&gt;

&lt;p&gt;My first project is to update the top of the page (“Crisis header”) with a new design. The new design looks like below.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/AlanFermat/AlanFermat.github.io/blob/master/assets/images/fb2.png?raw=true&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Based on the design I went through scoping, coding, polishing, dogfooding(let the member of the team to test the product I built), lodestone testing(let the third-party testers to test the product) and A/B testing(compare the user engagements of the new design with the old one). In the end, the experimentation showed a lot of positive signals on relevant metrics like user engagements and the team decided to ship this new header after discussion.&lt;/p&gt;

&lt;p&gt;My second project is to build a unified feed, a more comprehensive feed listing for the Crisis Page. Originally, only two types of posts were shown in the feed: offer help and request help. In the new design, I wanted to include the posts that provided information for the users. For example, a user could post about the traffic status of a road in the affected area without classifying the post as “Offering Help” or “Requesting Help.” The new design looks like below.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/AlanFermat/AlanFermat.github.io/blob/master/assets/images/fb3.png?raw=true&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In this project, I made the design on my own based on the parity in Facebook IOS/Android. I also went through scoping out the project, coding, polishing, and dogfooding(let the member of the team test the product I built). By the end of my internship, I did not start the lodestone testing(let the third-party testers to test the product) yet.&lt;/p&gt;

&lt;p&gt;But if given time, lodestone testing and experimentation would be the next steps for this project.&lt;/p&gt;

&lt;p&gt;Aside from my two main projects, I also helped the team build the Volunteering Quick Promotion(it’s a recommendation post that is inserted in the user’s news feed) which was a product the team aimed to launch in July.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/AlanFermat/AlanFermat.github.io/blob/master/assets/images/fb4.png?raw=true&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I also built the new Mark Safe Quick Promotion that already exists in Facebook IOS/Android. I built the parity in FBLite to improve the user experience in the mark-safe flow (which is letting their friends know that they are safe by clicking “I’m Safe” button). The old(left) and new(right) design of the Mark Safe Quick Promotion are as below.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/AlanFermat/AlanFermat.github.io/blob/master/assets/images/fb5.png?raw=true&quot; /&gt;&lt;/p&gt;

&lt;p&gt;During the summer, I met a lot of interesting people at FB and knew what to improve on my next journey. Keep on grinding!&lt;/p&gt;</content><author><name>Alan Yu</name></author><category term="blog" /><category term="Internship" /><category term="Facebook" /><category term="Job" /><category term="the Crisis Response" /><summary type="html">During my internship at Facebook in summer 2019, I learned a lot about engineering in real-world settings. I was assigned to the Crisis Response team. The core product the team works on is the Crisis Response Page. When natural disasters occur(for example, an earthquake), the users can use the page to know if their friends are marked themselves safe and if other people in the affected area are requesting or offering help.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/images/fb1.png" /></entry><entry><title type="html">TensorFlow - not another introduction II</title><link href="http://localhost:4000/LSTM/" rel="alternate" type="text/html" title="TensorFlow - not another introduction II" /><published>2019-01-18T00:00:00-06:00</published><updated>2019-01-18T00:00:00-06:00</updated><id>http://localhost:4000/LSTM</id><content type="html" xml:base="http://localhost:4000/LSTM/">&lt;h2 id=&quot;background&quot;&gt;Background&lt;/h2&gt;

&lt;p&gt;In this post, we will introduce a more interesting, state-of-art neural network model to the readers: Long-short-term-memory Network.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Long-short-term-memory network&lt;/em&gt; or &lt;em&gt;LSTM network&lt;/em&gt; for short is a special case in Recurrent Neural Network that tries to avoid the “vanishing gradient problem.”&lt;/p&gt;

&lt;p&gt;Lost in the jargons? Let’s take a step back and try to understand all the words in the above paragraph. &lt;a href=&quot;https://en.wikipedia.org/wiki/Recurrent_neural_network&quot;&gt;Recurrent Neural Network&lt;/a&gt; is a type of neural network that specializes in processing sequences of inputs via its storage of the internal states.&lt;/p&gt;

&lt;p&gt;A typical Recurrent Neural Network looks like the following.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/AlanFermat/AlanFermat.github.io/master/assets/images/lstm1.png&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;vanishing-gradient-problem&quot;&gt;Vanishing Gradient Problem&lt;/h2&gt;

&lt;p&gt;If we unroll the network above, we will see something like this,
&lt;img src=&quot;https://raw.githubusercontent.com/AlanFermat/AlanFermat.github.io/master/assets/images/lstm6.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Recall how we calculate the derivative in the backpropagation from an &lt;a href=&quot;https://imalanyu.com/Neural-Network/&quot;&gt;early post&lt;/a&gt;.&lt;/p&gt;

&lt;div class=&quot;side-by-side&quot;&gt;
	&lt;div class=&quot;toleft&quot;&gt;
		&lt;div style=&quot;margin-bottom: 100pt&quot;&gt;
			&lt;img class=&quot;image&quot; src=&quot;https://raw.githubusercontent.com/AlanFermat/AlanFermat.github.io/master/assets/images/lstm3.png&quot; alt=&quot;Alt Text&quot; /&gt;
			&lt;figcaption class=&quot;caption&quot;&gt;Derivative of logistic activation function&lt;/figcaption&gt;
		&lt;/div&gt;
		&lt;div padding-bottom=&quot;10&quot;&gt;
			&lt;img class=&quot;image&quot; src=&quot;https://raw.githubusercontent.com/AlanFermat/AlanFermat.github.io/master/assets/images/lstm2.png&quot; alt=&quot;Alt Text&quot; /&gt;
			&lt;figcaption class=&quot;caption&quot;&gt;Illustration for calculating derivatives in backprop&lt;/figcaption&gt;
		&lt;/div&gt;
	&lt;/div&gt;
	&lt;div class=&quot;toright&quot;&gt;
		&lt;p&gt;Note that the derivative of loss with respect to activation function is often less than 1.&lt;/p&gt;
		&lt;p&gt;Think about the derivative with respect to the logistic function on the left for example, we know that maximum value is reached when the activation function takes the value of 0.5 which is less than 1.&lt;/p&gt;
		&lt;p&gt;If we have a deep RNN and apply that multiplier constantly as we backpropagate the errors to the input layer, the derived gradient is approaching zero. &lt;/p&gt;
		&lt;p&gt;The error from the output layer may not be able to influence the upfront layers as they are too far.&lt;/p&gt;
		&lt;p&gt;This pattern would make the training stagnant as no correction would be made in the upfront layers due to the errors in the output.&lt;/p&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;You can see more detailed explanations of the vanishing gradient problem below.&lt;/p&gt;

&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/qO_NLVjD6zE&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;h2 id=&quot;lstm&quot;&gt;LSTM&lt;/h2&gt;

&lt;h3 id=&quot;lstm-module&quot;&gt;LSTM Module&lt;/h3&gt;
&lt;p&gt;Walla! Here comes the savior – LSTM network. In LSTM net, we don’t push the long term memory through the activation function, i.e. we don’t backpropagate the long term memory of an error all the way to the input layer. Thus, we don’t have a vanishing gradient problem!&lt;/p&gt;

&lt;p&gt;Let’s look at the LSTM “modules” as below.&lt;/p&gt;

&lt;p&gt;In the module,&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Vector travels along the lines&lt;/li&gt;
  &lt;li&gt;A circle/oval represents applications of a function to each dimension in a vector&lt;/li&gt;
  &lt;li&gt;A rectangle represents a neural network&lt;/li&gt;
  &lt;li&gt;𝞂 stands for a logistic activation layer at the top&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;tanh&lt;/em&gt; means a hyperbolic tangent activation layer at the top&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/AlanFermat/AlanFermat.github.io/master/assets/images/lstm4.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Note that each module accepts input at a particular time tick as well as state(“long term memory”) and last output(“short term memory”). Then the module will use all of them to output a view of its current state.&lt;/p&gt;

&lt;h3 id=&quot;a-single-lstm-unit&quot;&gt;A Single LSTM Unit&lt;/h3&gt;

&lt;p&gt;A single LSTM unit looks like this,&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/AlanFermat/AlanFermat.github.io/master/assets/images/lstm5.png&quot; /&gt;&lt;/p&gt;

&lt;div class=&quot;side-by-side&quot;&gt;
	&lt;div class=&quot;toleft&quot;&gt;
		&lt;img src=&quot;https://raw.githubusercontent.com/AlanFermat/AlanFermat.github.io/master/assets/images/lstm7.png&quot; /&gt;
	&lt;/div&gt;
	&lt;div class=&quot;toright&quot;&gt;
		&lt;p&gt;First we will run teh &quot;forget gate&quot;,&lt;/p&gt;
		&lt;ol&gt;
			&lt;li&gt;Last output and new input pass through a neural net with logistic layer at the top&lt;/li&gt;
			&lt;li&gt;it will produce all values from 0 to 1&lt;/li&gt;
		&lt;/ol&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;side-by-side&quot;&gt;
	&lt;div class=&quot;toleft&quot;&gt;
		&lt;img src=&quot;https://raw.githubusercontent.com/AlanFermat/AlanFermat.github.io/master/assets/images/lstm8.png&quot; /&gt;
	&lt;/div&gt;
	&lt;div class=&quot;toright&quot;&gt;
		&lt;p&gt;Then we run an &quot;input gate&quot;,&lt;/p&gt;
		&lt;ol&gt;
			&lt;li&gt;Input plus last output passes through two NNs
				&lt;ol&gt;
				&lt;li&gt;One with a tanh layer at the top producing a value from -1 to 1&lt;/li&gt;
				&lt;li&gt;One with logistic layer at the top producing a value from 0 to 1&lt;/li&gt;
				&lt;/ol&gt;
			&lt;/li&gt;
			&lt;li&gt;Item-by-item multiplication will produce a vector for the current state
				&lt;ol&gt;
					&lt;li&gt;The update is added to the long-term memory&lt;/li&gt;
				&lt;/ol&gt;
			&lt;/li&gt;
		&lt;/ol&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;side-by-side&quot;&gt;
	&lt;div class=&quot;toleft&quot;&gt;
		&lt;img src=&quot;https://raw.githubusercontent.com/AlanFermat/AlanFermat.github.io/master/assets/images/lstm9.png&quot; /&gt;
	&lt;/div&gt;
	&lt;div class=&quot;toright&quot;&gt;
		&lt;p&gt;In the end, we run &quot;output gate&quot;,&lt;/p&gt;
		&lt;ol&gt;
			&lt;li&gt;Push state through a tanh layer at the top producing a value from -1 to 1&lt;/li&gt;
		&lt;/ol&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;h2 id=&quot;code-snippets&quot;&gt;Code Snippets&lt;/h2&gt;

&lt;p&gt;With the help of Tensorflow framework, creating an LSTM unit is quite straightforward.&lt;/p&gt;

&lt;p&gt;Let’s take a brief walk-through.&lt;/p&gt;

&lt;p&gt;Set up a 1000-unit LSTM network.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;hiddenUniots = 1000
lstm = tf.nn.rnn_cell.LSTMCell(hiddenUnits)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Set up the initial state&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;initialState = lstm.zero_state(batchSize, tf.float32)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Train the LSTM net&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;currentState = initialState
for iter in range(maxSeqLen):
    timeTick = input[iter]
    # concatenate the state with the input, 
    # then compute the next state
    (lstm_output, currentState) = lstm(timeTick, currentState)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;where &lt;em&gt;input&lt;/em&gt; is your stacked input variables.&lt;/p&gt;

&lt;p&gt;Note that &lt;em&gt;lstm_output&lt;/em&gt; is your final output, you can do a feedforward or softmax afterward to regularize it.&lt;/p&gt;

&lt;p&gt;And…, that’s it.&lt;/p&gt;

&lt;p&gt;Within five lines you could set up an LSTM module and you can choose optimizer to train the model(how to do that? check out the &lt;a href=&quot;https://imalanyu.com/TensorFlow/&quot;&gt;TensorFlow Tutorial Post&lt;/a&gt;)&lt;/p&gt;

&lt;h2 id=&quot;more-about-lstm&quot;&gt;More About LSTM&lt;/h2&gt;

&lt;p&gt;If you wanna know more about LSTM, check out the following&lt;/p&gt;
&lt;ol&gt;
	&lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/1503.04069.pdf&quot;&gt;Original Paper&lt;/a&gt;&lt;/li&gt;
	&lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=WCUNPb-5EYI&quot;&gt;Video Tutorial&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;</content><author><name>Alan Yu</name></author><category term="blog" /><category term="Machine Learning" /><category term="Neural Network" /><category term="Google" /><category term="Computation Framework" /><summary type="html">Background</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/images/tensorflow.jpg" /></entry><entry><title type="html">TensorFlow - not another introduction I</title><link href="http://localhost:4000/TensorFlow/" rel="alternate" type="text/html" title="TensorFlow - not another introduction I" /><published>2018-12-06T00:00:00-06:00</published><updated>2018-12-06T00:00:00-06:00</updated><id>http://localhost:4000/TensorFlow</id><content type="html" xml:base="http://localhost:4000/TensorFlow/">&lt;h2 id=&quot;background&quot;&gt;Background&lt;/h2&gt;

&lt;p&gt;After reading my blog about neural networks, some readers had questions on how to avoid implementing the neural network from scratch. They found it very hard to do backpropagations by manually taking derivatives for each processing element at each layer in the neural network. The computation is time-consuming and could sometimes be totally messed up if you make a single mistake.&lt;/p&gt;

&lt;p&gt;Therefore, today I wanna introduce this amazing neural network framework by Google to you: TensorFlow. It’s a relatively high-level framework facilitating neural network implementations.&lt;/p&gt;

&lt;p&gt;Tensorflow helps define a computation graph where neural networks are structured and engineered in a particular way. People can tune parameters and visualize the computation graph so that the lower-level computation part of neural network is hidden (like taking nasty derivative in backpropagation).&lt;/p&gt;

&lt;p&gt;In the following blog, we will be introducing you tensorflow by two simple examples.&lt;/p&gt;

&lt;p&gt;Note that in this blog, I will walk you through the basics of Tensorflow mainly by 
these two examples (instead of throw out all the syntax and formulas).&lt;/p&gt;

&lt;h2 id=&quot;mlp-multi-layer-perceptron&quot;&gt;MLP (Multi-Layer Perceptron)&lt;/h2&gt;

&lt;p&gt;This is an illustration of how you would use Tensorflow using MNIST data set (a data set that classifies hand-written digits). The data has 784 dimensions (784 columns). The amount (number of rows) of training data and testing data is upon our choice. Because it’s a digit recognition data, we have 10 classes in total.&lt;/p&gt;

&lt;p&gt;We first import the data and the packages that are needed.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import tensorflow as tf
from tensorflow.examples.tutorials.mnist import input_data
mnist = input_data.read_data_sets('MNIST_data', one_hot=True)
import matplotlib.pyplot as plt
import numpy as np
import random as ran
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Now we separate the data into train and test sets and wrap the procedure up in one function&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;def train_test_data(train_num, test_num):
	x_train = mnist.train.images[:train_num,:]
	y_train = mnist.train.labels[:train_num,:]
	x_test = mnist.test.images[:test_num,:]
	y_test = mnist.test.labels[:test_num,:]
	return x_train, y_train, x_test, y_test
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;For visualization, we will use the following function to see what a “digit” looks like&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;def display_digit(num):
    print(y_train[num])
    label = y_train[num].argmax(axis=0)
    image = x_train[num].reshape([28,28])
    plt.title('Example: %d  Label: %d' % (num, label))
    plt.imshow(image, cmap=plt.get_cmap('gray_r'))
    plt.show()

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Just for fun you can use the above function to display a handwritten digit&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;x_train, y_train, x_test, y_test = train_test_data(55000, 1000)
display_digit(ran.randint(0, x_train.shape[0])) 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;You will get similar image as shown below&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/AlanFermat/AlanFermat.github.io/blob/master/assets/images/digit.png?raw=true&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Now we want to build our neural network. We know from the previous post that a neural network is basically a variation of matrix multiplication.&lt;/p&gt;

&lt;p&gt;By convention, Tensorflow will require people to have place holders for the data. Therefore, we will have our place holder for x (input data) and y (labels) as below.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;dimension = 784
numClass = 10
x_ = tf.placeholder(tf.float32, shape=[None, dimension])
y_ = tf.placeholder(tf.float32, shape=[None, numClass])
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;One interesting about Tensorflow is that we will define the workflow of a neural network before running anything.&lt;/p&gt;

&lt;p&gt;Here, we want to construct a neural network composed of one hidden layer with 28 hidden units.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;hiddenUnits = 28
W = tf.Variable(np.random.normal(0, 0.05, (dimension, hiddenUnits)), dtype=tf.float32)
b = tf.Variable(np.zeros((1, hiddenUnits)), dtype=tf.float32)
W2 = tf.Variable(np.random.normal (0, 0.05, (hiddenUnits, numClass)),dtype=tf.float32)
b2 = tf.Variable(np.zeros((1, numClass)), dtype=tf.float32)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;With the selection of proper transfer function, we can have our neural network as follows, (we use softmax function, in the end, to smoothly classify digits)&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;output0 = tf.tanh(tf.matmul(x_, W) + b) 
output1 = tf.matmul(output0, W2) + b2
y = tf.nn.softmax(output1)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Now we will define our loss function as follows&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;loss = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This function is taking the log of all our predictions y (whose values range from 0 to 1) and element-wise multiplying by the example’s true value y_. If the log function for each value is close to zero, it will make the value a large negative number (i.e., &lt;em&gt;-np.log(0.01) = 4.6&lt;/em&gt;), and if it is close to 1, it will make the value a small negative number (i.e., &lt;em&gt;-np.log(0.99) = 0.1&lt;/em&gt;).&lt;/p&gt;

&lt;p&gt;Then we will define our learning rate (this will be explored heuristically and empirically).
Finally, we will define our algorithm by optimizing (minimizing) loss function&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;trainingAlg = tf.train.GradientDescentOptimizer(0.02).minimize(loss)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We also want to compute the accuracy afterwards,&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))
accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;After telling our neural network how to train the data and optimize the weights, we can run the computation graph by starting a Tensorflow session. In the session, we shall first initialize all the variables (it’s like declaring them as nodes in the computation graph). Then we iteratively train the model with fixed training steps.&lt;/p&gt;

&lt;p&gt;Note that when we run the session, we have to map the dummy variable in the original computation graph to the actual data that we are interested in. For example, here x_ is a dummy variable, and we are actually training on data x_train so we will build a feed_dict for such substitution.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;train_step = 1000
with tf.session() as sess:
	sess.run(tf.global_variables_initializer())
	x_train, y_train, x_test, y_test = train_test_data(55000, 10000)
	for i in range(train_step+1):
    	sess.run(trainingAlg, feed_dict={x_: x_train, y_: y_train})
    	if i%100 == 0:
        	print('Training Step:' + str(i) + '  Accuracy =  ' + str(sess.run(accuracy, feed_dict={x_: x_test, y_: y_test})) + '  Loss = ' + str(sess.run(loss, {x_: x_train, y_: y_train})))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Just running the above example gives the output as follows:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Training Step:0  Accuracy =  0.0608  Loss = 2.319519
Training Step:100  Accuracy =  0.5714  Loss = 2.0317836
Training Step:200  Accuracy =  0.6556  Loss = 1.6440488
Training Step:300  Accuracy =  0.7084  Loss = 1.3017232
Training Step:400  Accuracy =  0.769  Loss = 1.0667019
Training Step:500  Accuracy =  0.8082  Loss = 0.90720826
Training Step:600  Accuracy =  0.8315  Loss = 0.7943547
Training Step:700  Accuracy =  0.8444  Loss = 0.7113519
Training Step:800  Accuracy =  0.8555  Loss = 0.6483375
Training Step:900  Accuracy =  0.8645  Loss = 0.59918994
Training Step:1000  Accuracy =  0.8721  Loss = 0.5599681
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Note that we only run 1000 steps to get this decent result, you can try to run more steps to get a much better result!&lt;/p&gt;

&lt;p&gt;In the end, we will visualize how we classify the 11th training example as follows. Here, variable “i” is just for naming files while “num” is the order of the training example we are looking at.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;def display_compare(num, i):
    x_train = mnist.train.images[num,:].reshape(1,784)
    y_train = mnist.train.labels[num,:]
    label = y_train.argmax()
    prediction = sess.run(y, feed_dict={x_: x_train}).argmax()
    plt.title('Prediction: %d Label: %d' % (prediction, label))
    plt.imshow(x_train.reshape([28,28]), cmap=plt.get_cmap('gray_r'))
    plt.savefig(&quot;pic &quot; + str(i) + &quot;.png&quot;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We will make a small tweak to the session&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    x_train, y_train, x_test, y_test = train_test_data(55000, 10000)
    for i in range(train_step+1):
        sess.run(trainingAlg, feed_dict={x_: x_train, y_: y_train})
        if i%100 == 0:
            display_compare(11, i)
            print('Training Step:' + str(i) + '  Accuracy =  ' 
                + str(sess.run(accuracy, feed_dict={x_: x_test, y_: y_test})) + '  Loss = ' + str(sess.run(loss, {x_: x_train, y_: y_train})))

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Now, it will save the middle process as “.png” file in your working directory. Running this, you should get something similar to the following (still 1000 iterations as before),&lt;/p&gt;

&lt;div&gt;
    Start with:
    &lt;img src=&quot;https://github.com/AlanFermat/AlanFermat.github.io/blob/master/assets/images/pic0.png?raw=true&quot; /&gt;
    After 100 Iterations:
    &lt;img src=&quot;https://github.com/AlanFermat/AlanFermat.github.io/blob/master/assets/images/pic100.png?raw=true&quot; /&gt;
    After 500 Iterations:
    &lt;img src=&quot;https://github.com/AlanFermat/AlanFermat.github.io/blob/master/assets/images/pic500.png?raw=true&quot; /&gt;
&lt;/div&gt;

&lt;p&gt;It seems this model can learn perfectly for “3” at 11th position after merely 100 iterations.&lt;/p&gt;

&lt;p&gt;Full code can be found &lt;a href=&quot;https://github.com/AlanFermat/Blogs/blob/master/TensorFlow/MLP.py&quot;&gt;here&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;lstm-long-short-term-memory&quot;&gt;LSTM (Long-short term memory)&lt;/h2&gt;

&lt;p&gt;See my next post!&lt;/p&gt;

&lt;h2 id=&quot;a-few-notes&quot;&gt;A Few Notes&lt;/h2&gt;

&lt;p&gt;There is an amazing online course by Stanford that will familiarize you with Tensorflow. The course is called CS20.&lt;/p&gt;

&lt;p&gt;You can check out the link &lt;a href=&quot;https://web.stanford.edu/class/cs20si/syllabus.html&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;</content><author><name>Alan Yu</name></author><category term="blog" /><category term="Machine Learning" /><category term="Neural Network" /><category term="Google" /><category term="Computation Framework" /><summary type="html">Background</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/images/tensorflow.jpg" /></entry><entry><title type="html">Twitter Higher and Lower game</title><link href="http://localhost:4000/twitter-game/" rel="alternate" type="text/html" title="Twitter Higher and Lower game" /><published>2018-04-01T00:00:00-05:00</published><updated>2018-04-01T00:00:00-05:00</updated><id>http://localhost:4000/twitter-game</id><content type="html" xml:base="http://localhost:4000/twitter-game/">&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/AlanFermat/twitterhigherlower/master/ingame.png&quot; alt=&quot;Screenshot&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;twitter-lower-and-higher&quot;&gt;Twitter Lower and Higher&lt;/h2&gt;
&lt;p&gt;This is a web game developed in 24 hours during Rice Hackathon that will allow users to guess which person has higher Twitter followers.&lt;/p&gt;

&lt;p&gt;In each round, you will be given two celebrities or famous organizations. You need to guess which one of the two has higher followers on twitter.&lt;/p&gt;

&lt;p&gt;If your answer is correct, you can go to the next round; otherwise, the game is over and you will receive your scores.&lt;/p&gt;

&lt;p&gt;Try to reach a higher score!&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/AlanFermat/twitterhigherlower&quot;&gt;Check it out&lt;/a&gt; here.
If you need some help, just &lt;a href=&quot;https://github.com/AlanFermat/twitterhigherlower/issues&quot;&gt;tell me&lt;/a&gt;.&lt;/p&gt;</content><author><name>Alan Yu</name></author><category term="project" /><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://raw.githubusercontent.com/AlanFermat/twitterhigherlower/master/twitter.png" /></entry><entry><title type="html">Self-organizing Map – unsupervised neural network</title><link href="http://localhost:4000/SOM/" rel="alternate" type="text/html" title="Self-organizing Map -- unsupervised neural network" /><published>2018-03-21T22:44:00-05:00</published><updated>2018-03-21T22:44:00-05:00</updated><id>http://localhost:4000/SOM</id><content type="html" xml:base="http://localhost:4000/SOM/">&lt;h2 id=&quot;what-is-unsupervised-learning&quot;&gt;What is unsupervised learning?&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Unsupervised_learning&quot;&gt;Unsupervised machine learning&lt;/a&gt; is the machine learning task of inferring a function that describes the structure of “unlabeled” data. The goal for unsupervised learning is to detect some pattern behind a mass of data without any prior knowledge about the data. Sounds like roman policier? That’s right!&lt;/p&gt;

&lt;p&gt;Since the beginning of this field, a lot of unsupervised learning algorithms has been developed and applied widely. Algorithms like K-Means, hierarchical clustering, and principal component analysis are crucial tools in industry and academia.&lt;/p&gt;

&lt;p&gt;Here is a useful &lt;a href=&quot;https://medium.com/machine-learning-for-humans/unsupervised-learning-f45587588294&quot;&gt;link&lt;/a&gt; that explains the concepts of unsupervised learning in more detail.&lt;/p&gt;

&lt;h2 id=&quot;what-is-a-self-organizing-map&quot;&gt;What is a self-organizing map?&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/a self-organizing_map&quot;&gt;a self-organizing map (SOM)
&lt;/a&gt; is an amazing neural network. The number of published research papers related to a self-organizing map ranked the first place from 2010 to 2015.&lt;/p&gt;

&lt;p&gt;In this neural network, the output layer is a grid in which each neuron represents a value that tries to approximate the corresponding input value. The interesting part is that each neuron is actually a weight that needs to be tuned throughout the training process. The goal for this neuron network is to learn the representation of all input data points on neurons of the output grid and then cluster the data via similarities between neurons.&lt;/p&gt;

&lt;div class=&quot;side-by-side&quot;&gt;
	&lt;div class=&quot;toleft&quot;&gt;
		&lt;img class=&quot;image&quot; src=&quot;https://github.com/AlanFermat/AlanFermat.github.io/blob/master/assets/images/som2.jpg?raw=true&quot; alt=&quot;Alt Text&quot; /&gt;
		&lt;figcaption class=&quot;caption&quot;&gt;Illustrion of SOM&lt;/figcaption&gt;
	&lt;/div&gt;
	&lt;div class=&quot;toright&quot;&gt;
		&lt;p&gt;The major parameter is the number of neurons in the grid, called prototypes. The initialization of weights, learning rates and stopping criteria are the same as those of other neural networks.&lt;/p&gt; 
		&lt;p&gt;As we can see from the left, each data point is mapped to a neuron that best approximates the value of that data point. Notice that the learning process is to adjust weights so that we minimize the &quot;distance&quot; between the input data point and the neuron that represents it. &lt;/p&gt;
	&lt;/div&gt;
	&lt;div class=&quot;toleft&quot;&gt;
		&lt;img class=&quot;image&quot; src=&quot;https://github.com/AlanFermat/AlanFermat.github.io/blob/master/assets/images/som3.jpg?raw=true&quot; alt=&quot;Alt Text&quot; /&gt;
		&lt;figcaption class=&quot;caption&quot;&gt;Topological preserving&lt;/figcaption&gt;
	&lt;/div&gt;
	&lt;div class=&quot;toright&quot;&gt;
		&lt;p&gt; The most magic part about a self-organizing map is that it is topologically preserved meaning that the neighbors of the original data point *A* will be mapped to the neighbors of *A's* representation in the output grid.&lt;/p&gt;
		&lt;p&gt; This property makes a self-organizing map unique among all clustering algorithms. Since topological preservation can help the neural network recognize the patterns easily when the number of clusters is huge, a self-organizing map is especially good at recognizing terrains.&lt;/p&gt;
	&lt;/div&gt;
	&lt;div class=&quot;toleft&quot;&gt;
		&lt;img class=&quot;image&quot; src=&quot;https://github.com/AlanFermat/AlanFermat.github.io/blob/master/assets/images/som4.jpg?raw=true&quot; alt=&quot;Alt Text&quot; /&gt;
		&lt;figcaption class=&quot;caption&quot;&gt;Mathematical formula&lt;/figcaption&gt;
	&lt;/div&gt;
	&lt;div class=&quot;toright&quot;&gt;
		&lt;p&gt; Here comes the mathematical formula for SOM. As we can see, one round of training consists of two steps. The first one is a competition algorithm where we will choose a prototype closest to the given pattern based on the information we have. At the second step, we update our weight based on a predefined distance of our prototype from the original pattern. &lt;/p&gt;
		&lt;p&gt;Note that the *h* function that is used in the updating is analogous to gradient descent in artificial neural networks. However, normally we will use Manhattan distance here instead of Euclidean to preserve the topological structure. &lt;/p&gt;
	&lt;/div&gt;

&lt;/div&gt;

&lt;p&gt;After the above explanation, I know some of you are still unclear about how it works or how the mathematical formula works, so do not hesitate to check out the following links:&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt; &lt;a href=&quot;http://www.pitt.edu/~is2470pb/Spring05/FinalProjects/Group1a/tutorial/som.html&quot;&gt;Lecture notes&lt;/a&gt;&lt;/li&gt;
	&lt;li&gt; &lt;a href=&quot;https://www.youtube.com/watch?v=LjJeT7rwvF4&quot;&gt;Lecture video (unrelated to the above link)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;can-you-show-me-an-example&quot;&gt;Can you show me an example?&lt;/h2&gt;

&lt;p&gt;First, we generate four sets of 3-dimensional random Gaussian vectors with 1000 points in each set, each with a variance of &lt;strong&gt;0.1&lt;/strong&gt;. Then we center the data sets at &lt;strong&gt;(0,0,0), (0,7,0), (7,0,0), and (7,7,0)&lt;/strong&gt;, respectively.&lt;/p&gt;

&lt;p&gt;We will train a SOM with 100 prototypes trying to cluster the data into 4 clusters.&lt;/p&gt;

&lt;p&gt;Source codes are uploaded &lt;a href=&quot;afs&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;div class=&quot;side-by-side&quot;&gt;
	&lt;div class=&quot;toleft&quot;&gt;
		&lt;img class=&quot;image&quot; src=&quot;https://github.com/AlanFermat/AlanFermat.github.io/blob/master/assets/images/som5.jpg?raw=true&quot; alt=&quot;Alt Text&quot; /&gt;
		&lt;figcaption class=&quot;caption&quot;&gt;Neural network parameters&lt;/figcaption&gt;
	&lt;/div&gt;
	&lt;div class=&quot;toright&quot;&gt;
		&lt;p&gt; We will use the parameters as suggested by the left to train and test the data. &lt;/p&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;side-by-side&quot;&gt;
	&lt;div class=&quot;toleft&quot;&gt;
		&lt;img class=&quot;image&quot; src=&quot;https://github.com/AlanFermat/AlanFermat.github.io/blob/master/assets/images/som6.jpg?raw=true&quot; alt=&quot;Alt Text&quot; /&gt;
		&lt;figcaption class=&quot;caption&quot;&gt;visualization of the grid after 250k training steps&lt;/figcaption&gt;
	&lt;/div&gt;
	&lt;div class=&quot;torights&quot;&gt;
		&lt;p&gt; Note that in this graph, blue dots are generated points that are to be clustered. Red dots represent prototypes For illustration, we connect them by black solid lines. &lt;/p&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;From the result, we can see that most prototypes are bundled and gathered to four corners. Because there is no cluster in the middle, the grid is stretched out tightly so that maximum of prototypes can match the data points optimally.&lt;/p&gt;

&lt;p&gt;Source code is &lt;a href=&quot;https://github.com/AlanFermat/Blogs/blob/master/SOM/SOM.m&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;reference&quot;&gt;Reference&lt;/h2&gt;

&lt;p&gt; http://www.cs.nott.ac.uk/~pszqiu/Teaching/2004/G5BNEC/G5BNEC-SOM.pdf &lt;/p&gt;
&lt;p&gt;https://link.springer.com/referenceworkentry/10.1007%2F978-3-540-92910-9_19&lt;/p&gt;
&lt;p&gt;https://www.sciencedirect.com/science/article/pii/S0925231298000307&lt;/p&gt;</content><author><name>Alan Yu</name></author><category term="blog" /><category term="Machine Learning" /><category term="Neural Network" /><category term="a self-organizing Map" /><category term="Unsupervised Learning" /><summary type="html">What is unsupervised learning?</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/images/som1.jpg" /></entry><entry><title type="html">Shelper</title><link href="http://localhost:4000/Shelper/" rel="alternate" type="text/html" title="Shelper" /><published>2017-09-22T00:00:00-05:00</published><updated>2017-09-22T00:00:00-05:00</updated><id>http://localhost:4000/Shelper</id><content type="html" xml:base="http://localhost:4000/Shelper/">&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/AlanFermat/Shelper/master/frontend/shelper.jpg&quot; alt=&quot;Screenshot&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;shelper&quot;&gt;Shelper&lt;/h2&gt;

&lt;p&gt;This is an API built by 4 members within 48 hours during Rice Hackathon.&lt;/p&gt;

&lt;p&gt;This API connects emergency shelters with people who are looking for help during Hurricane Harvey.&lt;/p&gt;

&lt;p&gt;I implemented a customized Google Map API (JSON and react.js) and visualized the live data on the screen from a database (MongoDB). We deployed this API as a web app via Flask.&lt;/p&gt;

&lt;p&gt;A user can use this app to login, either seeking for active shelters or donating necessities to the nearby shelters.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/AlanFermat/Shelper&quot;&gt;Check it out&lt;/a&gt; here.
If you need some help, just &lt;a href=&quot;https://github.com/AlanFermat/Shelper/issues&quot;&gt;tell me&lt;/a&gt;.&lt;/p&gt;</content><author><name>Alan Yu</name></author><category term="project" /><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://raw.githubusercontent.com/AlanFermat/Shelper/master/logo.png" /></entry><entry><title type="html">Neural Network – an introduction to function approximation</title><link href="http://localhost:4000/Neural-Network/" rel="alternate" type="text/html" title="Neural Network -- an introduction to function approximation" /><published>2017-07-22T00:00:00-05:00</published><updated>2017-07-22T00:00:00-05:00</updated><id>http://localhost:4000/Neural-Network</id><content type="html" xml:base="http://localhost:4000/Neural-Network/">&lt;h2 id=&quot;what-is-a-neural-network&quot;&gt;What is a neural network?&lt;/h2&gt;

&lt;h3 id=&quot;background&quot;&gt;Background&lt;/h3&gt;

&lt;p&gt;As computer science students, we often heard these fancy stuff in life: image classification, pattern recognition, convolutional neural network, machine learning, etc. Sometimes, we are so overwhelmed by the jargons in the field that we do not wanna explore the field ourselves.&lt;/p&gt;

&lt;p&gt;However, we are all human beings living in the 21st century – &lt;a href=&quot;https://www.tandfonline.com/doi/full/10.1080/20964471.2017.1397411&quot;&gt; the era of big data &lt;/a&gt;, making techniques such as machine learning useful and meaningful for data analysis.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Machine_learning&quot;&gt;Machine learning&lt;/a&gt; is a century-old field that recently became tremendously popular due to the demand for convenient tools facilitates data analysis. It has two intersecting subfields based on techniques, namely statistical machine learning and neural network. Whereas statistical machine learning involves a heavy load of statistics, neural networks stress more on designing and parameter-tuning.&lt;/p&gt;

&lt;p&gt;Based on the data we have, we can also divide machine learning into &lt;a href=&quot;https://en.wikipedia.org/wiki/Supervised_learning&quot;&gt;supervised&lt;/a&gt; and &lt;a href=&quot;https://en.wikipedia.org/wiki/Unsupervised_learning&quot;&gt;unsupervised learning&lt;/a&gt;. While supervised learning is to figure out the pattern with the guidance of a given standard, unsupervised learning is to recognize the pattern without any additional information.&lt;/p&gt;

&lt;p&gt;The neural network is one group of algorithms used for machine learning that models the data using graphs of artificial neurons. It tries to mimic how the neurons in human brains work.&lt;/p&gt;

&lt;p&gt;In this post, we will focus majorly on applying the neural network to function approximation. Since we will be informed of what our outputs should look like, we will be discussing about supervised neural network techniques only. Moreover, for the sake of illustration, we only focus on Artificial Neural Network architecture.&lt;/p&gt;

&lt;p&gt;For those who are interested in Long-short Term Memory Neural Network, Convolutional Neural Network, and Generative Adversarial Network, please check out the links below:&lt;/p&gt;

&lt;ul&gt; 
	&lt;li&gt; Long-short Term Memory: 
	&lt;a href=&quot;http://colah.github.io/posts/2015-08-Understanding-LSTMs/&quot;&gt;LSTM Blog&lt;/a&gt;
	&lt;a href=&quot;http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture10.pdf&quot;&gt;Stanford CS231n&lt;/a&gt;
	&lt;/li&gt;
	&lt;li&gt; Convolutional: 
	&lt;a href=&quot;https://www.google.com/search?q=convolutional+neural+network&amp;amp;rlz=1C5CHFA_enUS708US708&amp;amp;oq=convolutional&amp;amp;aqs=chrome.0.0j69i57j69i65j69i60j0l2.1802j1j9&amp;amp;sourceid=chrome&amp;amp;ie=UTF-8&quot;&gt; CNN Blog &lt;/a&gt;
	&lt;a href=&quot;http://cs231n.github.io/convolutional-networks/&quot;&gt;Stanford CS231n&lt;/a&gt;
	&lt;/li&gt;
	&lt;li&gt; Generative Adversarial: 
	&lt;a href=&quot;https://medium.com/@jonathan_hui/gan-why-it-is-so-hard-to-train-generative-advisory-networks-819a86b3750b&quot;&gt; GAN Blog&lt;/a&gt;
	&lt;a href=&quot;https://arxiv.org/abs/1406.2661&quot;&gt;Original Paper&lt;/a&gt;
	&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;technical-detail&quot;&gt;Technical detail&lt;/h3&gt;

&lt;h4 id=&quot;1-components&quot;&gt;1. Components&lt;/h4&gt;

&lt;div class=&quot;side-by-side&quot;&gt;
	&lt;div class=&quot;toleft&quot;&gt;
		&lt;img class=&quot;image&quot; src=&quot;https://github.com/AlanFermat/AlanFermat.github.io/blob/master/assets/images/nn1.jpg?raw=true&quot; alt=&quot;Alt Text&quot; /&gt;
		&lt;figcaption class=&quot;caption&quot;&gt;Illustration for one layer&lt;/figcaption&gt;
	&lt;/div&gt;
	&lt;div class=&quot;toright&quot;&gt;
		&lt;p&gt; A neuron network is a layer-by-layer structure. At each layer, it consists of processing elements (referred to as PEs afterward) and transfer functions. &lt;/p&gt;
		&lt;p&gt; Usually, the first layer of a network is called the input layer, the last layer is called the output layer and the layers in between are hidden layers. &lt;/p&gt;
		&lt;p&gt; The architecture of a neuron network is composed of the way that these layers combine together. For example, a network with 3-3-1 is a neural network where the first and second layer consist of 3 PEs and the output layer is of 1 PE. Note that the neural network on the left is of the topology N-1 with only two layers, one input layer, and one output layer. &lt;/p&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;Useful video explaning neural networks in more details:&lt;/p&gt;
&lt;iframe width=&quot;699&quot; height=&quot;393&quot; src=&quot;https://www.youtube.com/embed/aircAruvnKk&quot; frameborder=&quot;0&quot; allow=&quot;autoplay; encrypted-media&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;hr /&gt;

&lt;h4 id=&quot;2-forward-propagation&quot;&gt;2. Forward propagation&lt;/h4&gt;

&lt;div class=&quot;side-by-side&quot;&gt;
	&lt;div class=&quot;toleft&quot;&gt;
		&lt;img class=&quot;image&quot; src=&quot;https://github.com/AlanFermat/AlanFermat.github.io/blob/master/assets/images/nn2.jpg?raw=true&quot; alt=&quot;Alt Text&quot; /&gt;
		&lt;figcaption class=&quot;caption&quot;&gt;what does weight look like&lt;/figcaption&gt;
	&lt;/div&gt;
	&lt;div class=&quot;toright&quot;&gt;
		&lt;p&gt; Weight is a mathematical representation of how important a factor is in the neural network. Assuming the identity transfer function, the higher the value of weight is, the more effect of that weight will be taken into account when calculating the output of the current layer. &lt;/p&gt;
		&lt;p&gt; The weight in the form of the left is the weight of the first layer from jth PE to the ith PE in the next layer.&lt;/p&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;side-by-side&quot;&gt;
	&lt;div class=&quot;toleft&quot;&gt;
		&lt;img class=&quot;image&quot; src=&quot;https://github.com/AlanFermat/AlanFermat.github.io/blob/master/assets/images/nn3.jpg?raw=true&quot; alt=&quot;Alt Text&quot; /&gt;
		&lt;figcaption class=&quot;caption&quot;&gt;transfer function&lt;/figcaption&gt;
	&lt;/div&gt;
	&lt;div class=&quot;toright&quot;&gt;
		&lt;p&gt; An transfer function a node defines the output of that node given an input or set of inputs. &lt;/p&gt;
		&lt;p&gt; The simplest transfer function is identity function which gives an identity map from the output of the previous layer to the input of the next layer. Researchers and scientists normally choose sigmoid function or tanh function as transfer functions. However, the choice of the transfer function is open to discuss and their relative advantages are discussed &lt;a href=&quot;https://papers.nips.cc/paper/874-how-to-choose-an-transfer-function.pdf&quot;&gt;here&lt;/a&gt;. &lt;/p&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;In forward propagation, we will first aggregate the results calculated from the previous layer, applying transfer function as indicated above. We will do this layer by layer toward the output layer.&lt;/p&gt;

&lt;div class=&quot;side-by-side&quot;&gt;
	&lt;div class=&quot;toleft&quot;&gt;
		&lt;img class=&quot;image&quot; src=&quot;https://github.com/AlanFermat/AlanFermat.github.io/blob/master/assets/images/nn4.jpg?raw=true&quot; alt=&quot;Alt Text&quot; /&gt;
		&lt;figcaption class=&quot;caption&quot;&gt;Mathematical formula for forward propagation&lt;/figcaption&gt;
	&lt;/div&gt;
	&lt;div class=&quot;toright&quot;&gt;
		&lt;p&gt; The mathematic formula for forward propagation is shown on the left. You can iteratively do forward propagations for all layers. &lt;/p&gt;
		&lt;p&gt;We can also rewrite the summation part by matrix multiplication. We will leave this as an exercise for our readers. &lt;/p&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;Useful video for further illustration:&lt;/p&gt;
&lt;iframe width=&quot;699&quot; height=&quot;393&quot; src=&quot;https://www.youtube.com/embed/UJwK6jAStmg&quot; frameborder=&quot;0&quot; allow=&quot;autoplay; encrypted-media&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;h4 id=&quot;3-backward-propogation&quot;&gt;3. Backward propogation&lt;/h4&gt;

&lt;p&gt;When we reach the output layer in our neural network, we want to see how good our predicted result is compared to the desired output. So we will designate the network with an error function (usually Mean Square Error or Edit Distance) to evaluate how well we are doing with our current weights values and structures.&lt;/p&gt;

&lt;div class=&quot;side-by-side&quot;&gt;
	&lt;div class=&quot;toleft&quot;&gt;
		&lt;img class=&quot;image&quot; src=&quot;https://github.com/AlanFermat/AlanFermat.github.io/blob/master/assets/images/nn5.jpg?raw=true&quot; alt=&quot;Alt Text&quot; /&gt;
		&lt;figcaption class=&quot;caption&quot;&gt;Mathematical formula for backward propagation&lt;/figcaption&gt;
	&lt;/div&gt;
	&lt;div class=&quot;toright&quot;&gt;
		&lt;p&gt; We start from the output layer, comparing the desired results with predicted results then tracing back one layer at a time. &lt;/p&gt;
		&lt;p&gt; The adjustments are made to our weights through various methods where gradient descent is the most popular one. &lt;/p&gt;
		&lt;p&gt; Once we are done adjusting weights and reach the input layer, we will redo the forward propagation again. &lt;/p&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;Useful video for further illustration:&lt;/p&gt;
&lt;iframe width=&quot;640&quot; height=&quot;360&quot; src=&quot;https://www.youtube.com/embed/GlcnxUlrtek&quot; frameborder=&quot;0&quot; allow=&quot;autoplay; encrypted-media&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;h4 id=&quot;4-stopping-criteria&quot;&gt;4. Stopping criteria&lt;/h4&gt;

&lt;p&gt;Before we start training our neural network, we will pre-define a stopping criteria for our network. For example, we can say if the error is within 10e-5 then we will stop the training or if we have gone 10e5 iterations then we stop training further.&lt;/p&gt;

&lt;p&gt;Stopping criteria is crucial since we need to set a goal for our network to reach. There are various ways to determine where to stop. Check the links below:&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;&lt;a href=&quot;https://www.ibm.com/support/knowledgecenter/en/SS3RA7_15.0.0/com.ibm.spss.modeler.help/idh_neuralnet_stopping_rules.htm&quot;&gt;IBM blog&lt;/a&gt;&lt;/li&gt;
	&lt;li&gt;&lt;a href=&quot;http://image.diku.dk/shark/sphinx_pages/build/html/rest_sources/tutorials/first_steps/when_to_stop.html&quot;&gt;Shark developer post&lt;/a&gt;&lt;/li&gt;
	&lt;li&gt;&lt;a href=&quot;https://www.researchgate.net/post/How_do_I_know_when_to_stop_training_a_neural_network&quot;&gt;Research Gate Discussion Forum&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;5-implementation&quot;&gt;5. Implementation&lt;/h4&gt;

&lt;p&gt;I have a ready-to-go 3-layer neural network implemented in Matlab for you &lt;a href=&quot;https://github.com/AlanFermat/2R-robotic-arm-with-neural-network/blob/master/neuralNetwork.m&quot;&gt;here&lt;/a&gt;. It can be easily translated to Python using TensorFlow or pyTorch. You can also build your own neural networks.&lt;/p&gt;

&lt;ul&gt; 
	&lt;li&gt;A tutorial for TensorFlow users: &lt;a href=&quot;http://web.stanford.edu/class/cs20si/&quot;&gt; Stanford CS 20SI&lt;/a&gt;&lt;/li&gt;
	&lt;li&gt;Implement an ANN using pyTorch: &lt;a href=&quot;https://github.com/AlanFermat/Blogs/tree/master/ANNPytorch&quot;&gt; PyTorch Implementation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;what-is-function-approximation&quot;&gt;What is function approximation?&lt;/h2&gt;

&lt;p&gt;In general, a function approximation problem asks us to select a function among a well-defined class[clarification needed] that closely matches (“approximates”) a target function in a task-specific way.&lt;/p&gt;

&lt;p&gt;By &lt;a href=&quot;http://math.uchicago.edu/~may/REU2016/REUPapers/Gaddy.pdf&quot;&gt;Stone–Weierstrass theorem &lt;/a&gt;, every continuous function defined on a closed interval can be uniformly approximated as closely as desired by a polynomial function. We know from above that, if we choose linear function as our transfer function, then at each iteration of neural network, we are doing a matrix multiplication at each step.&lt;/p&gt;

&lt;p&gt;If we expand matrix multiplication, it is easy to see that the whole process of forward propogation is equivalent to using a polynomial to approximate the target function. By Stone-Weierstrass theorem, we have that neural network has the ability to approximate functions satisfying specific requirements (some constraints including continuity, domain, etc.).&lt;/p&gt;

&lt;p&gt;There is a formal theorem supporting this! The &lt;a href=&quot;https://en.wikipedia.org/wiki/Universal_approximation_theorem&quot;&gt;universal approximation theorem &lt;/a&gt; states that a feed-forward network with a single hidden layer containing a finite number of neurons can approximate continuous functions on compact subsets of n-dimensional Euclidean space, under mild assumptions on the transfer function. If you are interested in this theorem, you probably wanna read the following:&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;&lt;a href=&quot;https://blog.goodaudience.com/neural-networks-part-1-a-simple-proof-of-the-universal-approximation-theorem-b7864964dbd3&quot;&gt;blog&lt;/a&gt;&lt;/li&gt;
	&lt;li&gt;&lt;a href=&quot;http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.101.2647&amp;amp;rep=rep1&amp;amp;type=pdf&quot;&gt;Paper&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;can-you-show-me-an-example&quot;&gt;Can you show me an example?&lt;/h2&gt;

&lt;p&gt;We will illustrate the application of a simple 3-layer aritificial neural network in approximating inverse function &lt;strong&gt;f(x) = 1/x&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Source codes are uploaded &lt;a href=&quot;https://github.com/AlanFermat/Blogs/tree/master/OneOverX&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;div class=&quot;side-by-side&quot;&gt;
	&lt;div class=&quot;toleft&quot;&gt;
		&lt;img class=&quot;image&quot; src=&quot;https://github.com/AlanFermat/AlanFermat.github.io/blob/master/assets/images/nn6.jpg?raw=true&quot; alt=&quot;Alt Text&quot; /&gt;
		&lt;figcaption class=&quot;caption&quot;&gt;Neural network parameters&lt;/figcaption&gt;
	&lt;/div&gt;
	&lt;div class=&quot;toright&quot;&gt;
		&lt;p&gt; We will use the parameters as suggested by the left to train and test the data. &lt;/p&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;side-by-side&quot;&gt;
	&lt;div class=&quot;toleft&quot;&gt;
		&lt;img class=&quot;image&quot; src=&quot;https://github.com/AlanFermat/AlanFermat.github.io/blob/master/assets/images/nn7.jpg?raw=true&quot; alt=&quot;Alt Text&quot; /&gt;
		&lt;figcaption class=&quot;caption&quot;&gt;Error plot&lt;/figcaption&gt;
	&lt;/div&gt;
	&lt;div class=&quot;toright&quot;&gt;
		&lt;img class=&quot;image&quot; src=&quot;https://github.com/AlanFermat/AlanFermat.github.io/blob/master/assets/images/nn8.jpg?raw=true&quot; alt=&quot;Alt Text&quot; /&gt;
		&lt;figcaption class=&quot;caption&quot;&gt;Predicted VS Desired&lt;/figcaption&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;h2 id=&quot;what-are-other-applications&quot;&gt;What are other applications?&lt;/h2&gt;

&lt;p&gt;We also use a 3-layer neural network to teach a robotic arm how to draw certain pictures. The result is quite fun!&lt;/p&gt;

&lt;p&gt;Check this out: 
&lt;a href=&quot;https://github.com/AlanFermat/2R-robotic-arm-with-neural-network&quot;&gt; ANN application in Robotics &lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;reference&quot;&gt;Reference&lt;/h2&gt;
&lt;p&gt;https://www.tandfonline.com/doi/full/10.1080/20964471.2017.1397411&lt;/p&gt;

&lt;p&gt;http://math.uchicago.edu/~may/REU2016/REUPapers/Gaddy.pdf&lt;/p&gt;</content><author><name>Alan Yu</name></author><category term="blog" /><category term="Machine Learning" /><category term="Neural Network" /><category term="Function Approximation" /><category term="Supervised Learning" /><summary type="html">What is a neural network?</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/images/nnTitle.jpg" /></entry><entry><title type="html">Text mining for finding the best place to pursue medical research</title><link href="http://localhost:4000/text-mining-for-medical-research/" rel="alternate" type="text/html" title="Text mining for finding the best place to pursue medical research" /><published>2016-04-11T00:00:00-05:00</published><updated>2016-04-11T00:00:00-05:00</updated><id>http://localhost:4000/text-mining-for-medical-research</id><content type="html" xml:base="http://localhost:4000/text-mining-for-medical-research/">&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/AlanFermat/Text-mining-for-medical-research/master/wordcloud.jpg&quot; alt=&quot;Screenshot&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;text-mining-for-finding-the-best-place-pursuing-medical-research&quot;&gt;Text mining for finding the best place pursuing medical research&lt;/h2&gt;

&lt;p&gt;This is an open-end project that I worked with a team in a Statistic class at Rice University.&lt;/p&gt;

&lt;h3 id=&quot;where-is-the-best-place-for-medical-research&quot;&gt;Where is the best place for medical research?&lt;/h3&gt;

&lt;p&gt;We explore where the best places for health research and innovation are, and how this relates to the rates of specific illnesses.&lt;/p&gt;

&lt;p&gt;“Where” is meant to be general and refers not only to specific locations, such as cities/states, but also specific diseases, such as obesity, diabetes, and other characteristics. We then rank the 11 top cities for health research and innovation.&lt;/p&gt;

&lt;p&gt;In order to figure out where the best places are, we first analyze funding on three dimensions: type of disease, time, and location.&lt;/p&gt;

&lt;p&gt;Based on this high-level analysis, we find location is the most important factor. We deep dive into location, specifically city-level analysis.&lt;/p&gt;

&lt;p&gt;We breakdown the importance into three categories, which we call: access to talent, access to resources, and access to funding. Based on these three dimensions, we rank the top 11 cities. Based on our ranking system. New York is the top city, while Seattle is the bottom of the 11 cities.&lt;/p&gt;

&lt;p&gt;The results are presented beautifully via &lt;em&gt;ggplot&lt;/em&gt; and &lt;em&gt;wordcloud&lt;/em&gt; using R!&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/AlanFermat/Text-mining-for-medical-research&quot;&gt;Check it out&lt;/a&gt; here.
If you need some help, just &lt;a href=&quot;https://github.com/AlanFermat/Text-mining-for-medical-research/issues&quot;&gt;tell me&lt;/a&gt;.&lt;/p&gt;</content><author><name>Alan Yu</name></author><category term="project" /><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://raw.githubusercontent.com/AlanFermat/Text-mining-for-medical-research/master/data-mining.jpg" /></entry></feed>