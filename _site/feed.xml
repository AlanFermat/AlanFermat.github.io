<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.7.3">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2018-08-09T14:04:36-05:00</updated><id>http://localhost:4000/</id><title type="html">Alan Yu</title><subtitle>A blog about technology and stuff related</subtitle><entry><title type="html">Twitter Higher and Lower game</title><link href="http://localhost:4000/twitter-game/" rel="alternate" type="text/html" title="Twitter Higher and Lower game" /><published>2018-04-01T00:00:00-05:00</published><updated>2018-04-01T00:00:00-05:00</updated><id>http://localhost:4000/twitter-game</id><content type="html" xml:base="http://localhost:4000/twitter-game/">&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/AlanFermat/twitterhigherlower/master/ingame.png&quot; alt=&quot;Screenshot&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;twitter-lower-and-higher&quot;&gt;Twitter Lower and Higher&lt;/h2&gt;
&lt;p&gt;This is a web game developed in 24 hours during Rice Hackathon that will allow users to guess which person has a higher Twitter followers.&lt;/p&gt;

&lt;p&gt;In each round, you will be given two celebrities or famous organizations. You need to guess which one of the two has a higher followers on twitter.&lt;/p&gt;

&lt;p&gt;If your answer is correct, you can go to the next round; otherwise, the game is over and you will receive your scores.&lt;/p&gt;

&lt;p&gt;Try to reach a higher score!&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/AlanFermat/twitterhigherlower&quot;&gt;Check it out&lt;/a&gt; here.
If you need some help, just &lt;a href=&quot;https://github.com/AlanFermat/twitterhigherlower/issues&quot;&gt;tell me&lt;/a&gt;.&lt;/p&gt;</content><author><name>Alan Yu</name></author><category term="project" /><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://raw.githubusercontent.com/AlanFermat/twitterhigherlower/master/twitter.png" /></entry><entry><title type="html">Self-organized Map – unsupervised neural network</title><link href="http://localhost:4000/SOM/" rel="alternate" type="text/html" title="Self-organized Map -- unsupervised neural network" /><published>2018-03-21T22:44:00-05:00</published><updated>2018-03-21T22:44:00-05:00</updated><id>http://localhost:4000/SOM</id><content type="html" xml:base="http://localhost:4000/SOM/"></content><author><name>Alan Yu</name></author><category term="blog" /><category term="Machine Learning" /><category term="Neural Network" /><category term="Self-organized Map" /><category term="Unsupervised Learning" /><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/images/som1.jpg" /></entry><entry><title type="html">Shelper</title><link href="http://localhost:4000/Shelper/" rel="alternate" type="text/html" title="Shelper" /><published>2017-09-22T00:00:00-05:00</published><updated>2017-09-22T00:00:00-05:00</updated><id>http://localhost:4000/Shelper</id><content type="html" xml:base="http://localhost:4000/Shelper/">&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/AlanFermat/Shelper/master/frontend/shelper.jpg&quot; alt=&quot;Screenshot&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;shelper&quot;&gt;Shelper&lt;/h2&gt;

&lt;p&gt;This is an API built by 4 members within 48 hours during Rice Hackathon.&lt;/p&gt;

&lt;p&gt;This API connects emergency shelters with people who are looking for help during Hurricane Harvey.&lt;/p&gt;

&lt;p&gt;I implemented customized Google Map API (JSON and react.js) and visualized the live data on the screen from database (MongoDB). We deployed this API as a web app via Flask.&lt;/p&gt;

&lt;p&gt;A user can use this app to login, either seeking for active shelters or donating necessities to the nearby shelters.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/AlanFermat/Shelper&quot;&gt;Check it out&lt;/a&gt; here.
If you need some help, just &lt;a href=&quot;https://github.com/AlanFermat/Shelper/issues&quot;&gt;tell me&lt;/a&gt;.&lt;/p&gt;</content><author><name>Alan Yu</name></author><category term="project" /><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://raw.githubusercontent.com/AlanFermat/Shelper/master/logo.png" /></entry><entry><title type="html">Neural Network – an introduction to function approximation</title><link href="http://localhost:4000/Neural-Network/" rel="alternate" type="text/html" title="Neural Network -- an introduction to function approximation" /><published>2017-07-22T00:00:00-05:00</published><updated>2017-07-22T00:00:00-05:00</updated><id>http://localhost:4000/Neural-Network</id><content type="html" xml:base="http://localhost:4000/Neural-Network/">&lt;h2 id=&quot;what-is-neural-network&quot;&gt;What is neural network?&lt;/h2&gt;

&lt;h3 id=&quot;background&quot;&gt;Background&lt;/h3&gt;

&lt;p&gt;As computer sciecne students, we often heard these fancy stuffs in life: image classification, pattern recognition, convolutional neural network, machine learning, etc. Sometimes, we are so overwhelmed by the jargons in the field that we do not wanna explore the field ourselves.&lt;/p&gt;

&lt;p&gt;However, we are all human beings living in the 21st century – &lt;a href=&quot;https://www.tandfonline.com/doi/full/10.1080/20964471.2017.1397411&quot;&gt; the era of big data &lt;/a&gt;, making techniques such as machine learning useful and meaningful for data analysis.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Machine_learning&quot;&gt;Machine learning&lt;/a&gt; is a century-old field that recently became tremendously popular due to the demand for convenient tools facilitates data analysis. It has two intersecting subfields based on techniques, namely statistical machine lerning and neural network. Whereas statistical machine learning involves a heavy load of statistics, neural networks stress more on designing and parameter-tuning.&lt;/p&gt;

&lt;p&gt;Based on the data we have, we can also divide machine learning into &lt;a href=&quot;https://en.wikipedia.org/wiki/Supervised_learning&quot;&gt;supervised&lt;/a&gt; and &lt;a href=&quot;https://en.wikipedia.org/wiki/Unsupervised_learning&quot;&gt;unsupervised learning&lt;/a&gt;. While supervised learning is to figure out the pattern with the guidance of a given standard, unsupervised learning is to recognize the pattern without any additional information.&lt;/p&gt;

&lt;p&gt;Neural network is one group of algorithms used for machine learning that models the data using graphs of artificial neurons. It tries to mimic how the neurons in human brains work.&lt;/p&gt;

&lt;p&gt;In this post, we will focus majorly on applying neural network to function approximation. Since we will be informed of what our outputs should look like, we will be discussing about supervised neural network techniques only. Moreover, for the sake of illustration we only focus on Artificial Neural Network architecture.&lt;/p&gt;

&lt;p&gt;For those who are interested in Long-short Term Memory Neural Network, Convolutional Neural Network and Generative Adversarial Network, please check out the links below:&lt;/p&gt;

&lt;ul&gt; 
	&lt;li&gt; Long-short Term Memory: 
	&lt;a href=&quot;http://colah.github.io/posts/2015-08-Understanding-LSTMs/&quot;&gt;LSTM Blog&lt;/a&gt;
	&lt;a href=&quot;http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture10.pdf&quot;&gt;Stanford CS231n&lt;/a&gt;
	&lt;/li&gt;
	&lt;li&gt; Convolutional: 
	&lt;a href=&quot;https://www.google.com/search?q=convolutional+neural+network&amp;amp;rlz=1C5CHFA_enUS708US708&amp;amp;oq=convolutional&amp;amp;aqs=chrome.0.0j69i57j69i65j69i60j0l2.1802j1j9&amp;amp;sourceid=chrome&amp;amp;ie=UTF-8&quot;&gt; CNN Blog &lt;/a&gt;
	&lt;a href=&quot;http://cs231n.github.io/convolutional-networks/&quot;&gt;Stanford CS231n&lt;/a&gt;
	&lt;/li&gt;
	&lt;li&gt; Generative Adversarial: 
	&lt;a href=&quot;https://medium.com/@jonathan_hui/gan-why-it-is-so-hard-to-train-generative-advisory-networks-819a86b3750b&quot;&gt; GAN Blog&lt;/a&gt;
	&lt;a href=&quot;https://arxiv.org/abs/1406.2661&quot;&gt;Original Paper&lt;/a&gt;
	&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;technical-detail&quot;&gt;Technical detail&lt;/h3&gt;

&lt;h4 id=&quot;1-components&quot;&gt;1. Components&lt;/h4&gt;

&lt;div class=&quot;side-by-side&quot;&gt;
	&lt;div class=&quot;toleft&quot;&gt;
		&lt;img class=&quot;image&quot; src=&quot;https://raw.githubusercontent.com/AlanFermat/AlanFermat.github.io/master/assets/images/nn1.jpg?token=AVJwLHHBZFQv6UECD-7Pd4Ip9VwGssqSks5bdb3YwA%3D%3D&quot; alt=&quot;Alt Text&quot; /&gt;
		&lt;figcaption class=&quot;caption&quot;&gt;Illustration for one layer&lt;/figcaption&gt;
	&lt;/div&gt;
	&lt;div class=&quot;toright&quot;&gt;
		&lt;p&gt; A neuron network is a layer-by-layer structure. At each layer, it consists of processing elements (referred as PEs afterwards) and transfer functions. &lt;/p&gt;
		&lt;p&gt; Usually, the first layer of a network is called input layer, the last layer is called output layer and the layers in between are hidden layers. &lt;/p&gt;
		&lt;p&gt; The architecture of a neuron network is composed of the way that these layers combine together. For example, a network with 3-3-1 is a neural network where the first and second layer consist of 3 PEs and the output layer is of 1 PE. Note that the neural network on the left is of the topology N-1 with only two layers, one input layer and one output layer. &lt;/p&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;Useful video explaning neural networks in more details:&lt;/p&gt;
&lt;iframe width=&quot;699&quot; height=&quot;393&quot; src=&quot;https://www.youtube.com/embed/aircAruvnKk&quot; frameborder=&quot;0&quot; allow=&quot;autoplay; encrypted-media&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;hr /&gt;

&lt;h4 id=&quot;2-forward-propagation&quot;&gt;2. Forward propagation&lt;/h4&gt;

&lt;div class=&quot;side-by-side&quot;&gt;
	&lt;div class=&quot;toleft&quot;&gt;
		&lt;img class=&quot;image&quot; src=&quot;https://raw.githubusercontent.com/AlanFermat/AlanFermat.github.io/master/assets/images/nn2.jpg?token=AVJwLGguShLYI_cgDH-xisVwleByhtfAks5bdb3swA%3D%3D&quot; alt=&quot;Alt Text&quot; /&gt;
		&lt;figcaption class=&quot;caption&quot;&gt;what does weight look like&lt;/figcaption&gt;
	&lt;/div&gt;
	&lt;div class=&quot;toright&quot;&gt;
		&lt;p&gt; Weight is a mathematical representation of how important a factor is in the neural network. Assuming the identity transfer function, the higher the value of weight is, the more effect of that weight will be taken into account when calculating the output of the current layer. &lt;/p&gt;
		&lt;p&gt; The weight in the form of the left is the weight of the first layer from jth PE to the ith PE in the next layer.&lt;/p&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;side-by-side&quot;&gt;
	&lt;div class=&quot;toleft&quot;&gt;
		&lt;img class=&quot;image&quot; src=&quot;https://raw.githubusercontent.com/AlanFermat/AlanFermat.github.io/master/assets/images/nn3.jpg?token=AVJwLDtliDrjnV7A7zs7noag5ZFYJErHks5bdb39wA%3D%3D&quot; alt=&quot;Alt Text&quot; /&gt;
		&lt;figcaption class=&quot;caption&quot;&gt;transfer function&lt;/figcaption&gt;
	&lt;/div&gt;
	&lt;div class=&quot;toright&quot;&gt;
		&lt;p&gt; An transfer function a node defines the output of that node given an input or set of inputs. &lt;/p&gt;
		&lt;p&gt; The simplest transfer function is identity function which gives an identity map from the output of the previous layer to the input of the next layer. Researchers and scientists normally choose sigmoid function or tanh function as transfer functions. However, the choice of the transfer function is open to discuss and their relative advantages are discussed &lt;a href=&quot;https://papers.nips.cc/paper/874-how-to-choose-an-transfer-function.pdf&quot;&gt;here&lt;/a&gt;. &lt;/p&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;In forward propagation, we will first aggregate the results calculated from the previous layer, applying transfer function as indicated above. We will do this layer by layer toward the output layer.&lt;/p&gt;

&lt;div class=&quot;side-by-side&quot;&gt;
	&lt;div class=&quot;toleft&quot;&gt;
		&lt;img class=&quot;image&quot; src=&quot;https://raw.githubusercontent.com/AlanFermat/AlanFermat.github.io/master/assets/images/nn4.jpg?token=AVJwLPjNyGu_roaVx5FNAFYt0Yk8udkqks5bdb4JwA%3D%3D&quot; alt=&quot;Alt Text&quot; /&gt;
		&lt;figcaption class=&quot;caption&quot;&gt;Mathematical formula for forward propagation&lt;/figcaption&gt;
	&lt;/div&gt;
	&lt;div class=&quot;toright&quot;&gt;
		&lt;p&gt; The mathematic formula for forward propagation is shown on the left. You can iteratively do forward propagations for all layers. &lt;/p&gt;
		&lt;p&gt;We can also rewrite the summation part by matrix multiplication. We will leave this as an exercise for our readers. &lt;/p&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;Useful video for further illustration:&lt;/p&gt;
&lt;iframe width=&quot;699&quot; height=&quot;393&quot; src=&quot;https://www.youtube.com/embed/UJwK6jAStmg&quot; frameborder=&quot;0&quot; allow=&quot;autoplay; encrypted-media&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;h4 id=&quot;3-backward-propogation&quot;&gt;3. Backward propogation&lt;/h4&gt;

&lt;p&gt;When we reach the output layer in our neural network, we want to see how good our predicted result is compared to the desired output. So we will designate the network with an error function (usually Mean Square Error or Edit Distance) to evaluate how well we are doing with our current weights values and structures.&lt;/p&gt;

&lt;div class=&quot;side-by-side&quot;&gt;
	&lt;div class=&quot;toleft&quot;&gt;
		&lt;img class=&quot;image&quot; src=&quot;https://raw.githubusercontent.com/AlanFermat/AlanFermat.github.io/master/assets/images/nn5.jpg?token=AVJwLKIsW97pVNrSeFTJIVHVX3OGTIvrks5bdb4XwA%3D%3D&quot; alt=&quot;Alt Text&quot; /&gt;
		&lt;figcaption class=&quot;caption&quot;&gt;Mathematical formula for forward propagation&lt;/figcaption&gt;
	&lt;/div&gt;
	&lt;div class=&quot;toright&quot;&gt;
		&lt;p&gt; We start from the output layer, comparing the desired results with predicted results then tracing back one layer at a time. &lt;/p&gt;
		&lt;p&gt; The adjustments are made to our weights through various methods where gradient descent is the most popular one. &lt;/p&gt;
		&lt;p&gt; Once we are done adjusting weights and reach the input layer, we will redo the forward propagation again. &lt;/p&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;Useful video for further illustration:&lt;/p&gt;
&lt;iframe width=&quot;640&quot; height=&quot;360&quot; src=&quot;https://www.youtube.com/embed/GlcnxUlrtek&quot; frameborder=&quot;0&quot; allow=&quot;autoplay; encrypted-media&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;h4 id=&quot;4-stopping-criteria&quot;&gt;4. Stopping criteria&lt;/h4&gt;

&lt;p&gt;Before we start training our neural network, we will pre-define a stopping criteria for our network. For example, we can say if the error is within 10e-5 then we will stop the training or if we have gone 10e5 iterations then we stop training further.&lt;/p&gt;

&lt;p&gt;Stopping criteria is crucial since we need to set a goal for our network to reach. There are various ways to determine where to stop. Check the links below:&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;&lt;a href=&quot;https://www.ibm.com/support/knowledgecenter/en/SS3RA7_15.0.0/com.ibm.spss.modeler.help/idh_neuralnet_stopping_rules.htm&quot;&gt;IBM blog&lt;/a&gt;&lt;/li&gt;
	&lt;li&gt;&lt;a href=&quot;http://image.diku.dk/shark/sphinx_pages/build/html/rest_sources/tutorials/first_steps/when_to_stop.html&quot;&gt;Shark developer post&lt;/a&gt;&lt;/li&gt;
	&lt;li&gt;&lt;a href=&quot;https://www.researchgate.net/post/How_do_I_know_when_to_stop_training_a_neural_network&quot;&gt;Research Gate Discussion Forum&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;5-implementation&quot;&gt;5. Implementation&lt;/h4&gt;

&lt;p&gt;I have a ready-to-go 3-layer neural network implemented in Matlab for you &lt;a href=&quot;https://github.com/AlanFermat/2R-robotic-arm-with-neural-network/blob/master/neuralNetwork.m&quot;&gt;here&lt;/a&gt;. It can be easily translated to Python using TensorFlow or pyTorch. You can also build your own neural networks.&lt;/p&gt;

&lt;ul&gt; 
	&lt;li&gt;A tutorial for TensorFlow users: &lt;a href=&quot;http://web.stanford.edu/class/cs20si/&quot;&gt; Stanford CS 20SI&lt;/a&gt;&lt;/li&gt;
	&lt;li&gt;Implement an ANN using pyTorch: &lt;a href=&quot;https://github.com/AlanFermat/Blogs/tree/master/ANNPytorch&quot;&gt; PyTorch Implementation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;what-is-function-approximation&quot;&gt;What is function approximation?&lt;/h2&gt;

&lt;p&gt;In general, a function approximation problem asks us to select a function among a well-defined class[clarification needed] that closely matches (“approximates”) a target function in a task-specific way.&lt;/p&gt;

&lt;p&gt;By &lt;a href=&quot;http://math.uchicago.edu/~may/REU2016/REUPapers/Gaddy.pdf&quot;&gt;Stone–Weierstrass theorem &lt;/a&gt;, every continuous function defined on a closed interval can be uniformly approximated as closely as desired by a polynomial function. We know from above that, if we choose linear function as our transfer function, then at each iteration of neural network, we are doing a matrix multiplication at each step.&lt;/p&gt;

&lt;p&gt;If we expand matrix multiplication, it is easy to see that the whole process of forward propogation is equivalent to using a polynomial to approximate the target function. By Stone-Weierstrass theorem, we have that neural network has the ability to approximate functions satisfying specific requirements (some constraints including continuity, domain, etc.).&lt;/p&gt;

&lt;p&gt;There is a formal theorem supporting this! The &lt;a href=&quot;https://en.wikipedia.org/wiki/Universal_approximation_theorem&quot;&gt;universal approximation theorem &lt;/a&gt; states that a feed-forward network with a single hidden layer containing a finite number of neurons can approximate continuous functions on compact subsets of n-dimensional Euclidean space, under mild assumptions on the transfer function. If you are interested in this theorem, you probably wanna read the following:&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;&lt;a href=&quot;https://blog.goodaudience.com/neural-networks-part-1-a-simple-proof-of-the-universal-approximation-theorem-b7864964dbd3&quot;&gt;blog&lt;/a&gt;&lt;/li&gt;
	&lt;li&gt;&lt;a href=&quot;http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.101.2647&amp;amp;rep=rep1&amp;amp;type=pdf&quot;&gt;Paper&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;what-is-an-example&quot;&gt;What is an example?&lt;/h2&gt;

&lt;p&gt;We will illustrate the application of a simple 3-layer aritifical neural network in approximating inverse function &lt;strong&gt;f(x) = 1/x&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Source codes are uploaded &lt;a href=&quot;https://github.com/AlanFermat/Blogs/tree/master/OneOverX&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;div class=&quot;side-by-side&quot;&gt;
	&lt;div class=&quot;toleft&quot;&gt;
		&lt;img class=&quot;image&quot; src=&quot;https://raw.githubusercontent.com/AlanFermat/AlanFermat.github.io/master/assets/images/nn6.jpg?token=AVJwLHJIsRM96GtAuJpZ0vVE1cDVIFYsks5bdcP0wA%3D%3D&quot; alt=&quot;Alt Text&quot; /&gt;
		&lt;figcaption class=&quot;caption&quot;&gt;Neural network parameters&lt;/figcaption&gt;
	&lt;/div&gt;
	&lt;div class=&quot;toright&quot;&gt;
		&lt;p&gt; We will use the parameters as suggested by the left to train and test the data. &lt;/p&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;side-by-side&quot;&gt;
	&lt;div class=&quot;toleft&quot;&gt;
		&lt;img class=&quot;image&quot; src=&quot;https://raw.githubusercontent.com/AlanFermat/AlanFermat.github.io/master/assets/images/nn7.jpg?token=AVJwLBuzL9xSJPo5uRp9re3QcQoSZTOjks5bdcQCwA%3D%3D&quot; alt=&quot;Alt Text&quot; /&gt;
		&lt;figcaption class=&quot;caption&quot;&gt;Error plot&lt;/figcaption&gt;
	&lt;/div&gt;
	&lt;div class=&quot;toright&quot;&gt;
		&lt;img class=&quot;image&quot; src=&quot;https://raw.githubusercontent.com/AlanFermat/AlanFermat.github.io/master/assets/images/nn8.jpg?token=AVJwLMu4W3cpd_kX4uJw-m8EA737kUNNks5bdcQOwA%3D%3D&quot; alt=&quot;Alt Text&quot; /&gt;
		&lt;figcaption class=&quot;caption&quot;&gt;Predicted VS Desired&lt;/figcaption&gt;
	&lt;/div&gt;
&lt;/div&gt;

&lt;h2 id=&quot;other-applications&quot;&gt;Other applications&lt;/h2&gt;

&lt;p&gt;Use 3-layer neural network to teach a robotic arm how to draw certain pictures.&lt;/p&gt;

&lt;p&gt;Check this out: 
&lt;a href=&quot;https://github.com/AlanFermat/2R-robotic-arm-with-neural-network&quot;&gt; ANN application in Robotics &lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;reference&quot;&gt;Reference&lt;/h2&gt;
&lt;p&gt;https://www.tandfonline.com/doi/full/10.1080/20964471.2017.1397411&lt;/p&gt;

&lt;p&gt;http://math.uchicago.edu/~may/REU2016/REUPapers/Gaddy.pdf&lt;/p&gt;</content><author><name>Alan Yu</name></author><category term="blog" /><category term="Machine Learning" /><category term="Neural Network" /><category term="Function Approximation" /><category term="Supervised Learning" /><summary type="html">What is neural network?</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/images/nnTitle.jpg" /></entry><entry><title type="html">Text mining for finding the best place pursuing medical research</title><link href="http://localhost:4000/text-mining-for-medical-research/" rel="alternate" type="text/html" title="Text mining for finding the best place pursuing medical research" /><published>2016-04-11T00:00:00-05:00</published><updated>2016-04-11T00:00:00-05:00</updated><id>http://localhost:4000/text-mining-for-medical-research</id><content type="html" xml:base="http://localhost:4000/text-mining-for-medical-research/">&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/AlanFermat/Text-mining-for-medical-research/master/wordcloud.jpg&quot; alt=&quot;Screenshot&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;text-mining-for-finding-the-best-place-pursuing-medical-research&quot;&gt;Text mining for finding the best place pursuing medical research&lt;/h2&gt;

&lt;h3 id=&quot;where-is-the-best-place-for-medical-research&quot;&gt;Where is the best place for medical research?&lt;/h3&gt;

&lt;p&gt;We explore where the best places for health research and innovation are, and how this relates to the rates of specific illnesses.&lt;/p&gt;

&lt;p&gt;“Where” is meant to be general and refers not only to specific locations, such as cities/states, but also specific diseases, such as obesity, diabetes, and other characteristics. We then rank the 11 top cities for health research and innovation.&lt;/p&gt;

&lt;p&gt;In order to figure out where the best places are, we first analyze funding on three dimensions: type of disease, time, and location.&lt;/p&gt;

&lt;p&gt;Based on this high-level analysis, we find location is the most important factor. We deep dive into location, specifically city-level analysis.&lt;/p&gt;

&lt;p&gt;We breakdown the importance into three categories, which we call: access to talent, access to resources, and access to funding. Based on these three dimensions, we rank the top 11 cities. Based on our ranking system. New York is the top city, while Seattle is the bottom among the 11 cities.&lt;/p&gt;

&lt;p&gt;The results are presented beautifully via ggplot and wordcloud using R!&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/AlanFermat/Text-mining-for-medical-research&quot;&gt;Check it out&lt;/a&gt; here.
If you need some help, just &lt;a href=&quot;https://github.com/AlanFermat/Text-mining-for-medical-research/issues&quot;&gt;tell me&lt;/a&gt;.&lt;/p&gt;</content><author><name>Alan Yu</name></author><category term="project" /><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://raw.githubusercontent.com/AlanFermat/Text-mining-for-medical-research/master/data-mining.jpg" /></entry></feed>